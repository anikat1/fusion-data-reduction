{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3976f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import random \n",
    "import tqdm\n",
    "\n",
    "import adios2 as ad2\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a792e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install adios2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6376d10",
   "metadata": {},
   "source": [
    "#Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dae69ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_f0(istep, expdir=None, iphi=None, inode=0, nnodes=None, average=False, \n",
    "            randomread=0.0, nchunk=16, fieldline=False):\n",
    "    \"\"\"\n",
    "    Read XGC f0 data\n",
    "    \"\"\"\n",
    "    def adios2_get_shape(f, varname):\n",
    "        nstep = int(f.available_variables()[varname]['AvailableStepsCount'])\n",
    "        shape = f.available_variables()[varname]['Shape']\n",
    "        lshape = None\n",
    "        if shape == '':\n",
    "            ## Accessing Adios1 file\n",
    "            ## Read data and figure out\n",
    "            v = f.read(varname)\n",
    "            lshape = v.shape\n",
    "        else:\n",
    "            lshape = tuple([ int(x.strip(',')) for x in shape.strip().split() ])\n",
    "        return (nstep, lshape)\n",
    "\n",
    "    fname = os.path.join(expdir, 'restart_dir/xgc.f0.%05d.bp'%istep)\n",
    "    if randomread > 0.0:\n",
    "        ## prefetch to get metadata\n",
    "        with ad2.open(fname, 'r') as f:\n",
    "            nstep, nsize = adios2_get_shape(f, 'i_f')\n",
    "            ndim = len(nsize)\n",
    "            nphi = nsize[0]\n",
    "            _nnodes = nsize[2] if nnodes is None else nnodes\n",
    "            nmu = nsize[1]\n",
    "            nvp = nsize[3]\n",
    "        assert _nnodes%nchunk == 0\n",
    "        _lnodes = list(range(inode, inode+_nnodes, nchunk))\n",
    "        lnodes = random.sample(_lnodes, k=int(len(_lnodes)*randomread))\n",
    "        lnodes = np.sort(lnodes)\n",
    "\n",
    "        lf = list()\n",
    "        li = list()\n",
    "        for i in tqdm(lnodes):\n",
    "            li.append(np.array(range(i,i+nchunk), dtype=np.int32))\n",
    "            with ad2.open(fname, 'r') as f:\n",
    "                nphi = nsize[0] if iphi is None else 1\n",
    "                iphi = 0 if iphi is None else iphi\n",
    "                start = (iphi,0,i,0)\n",
    "                count = (nphi,nmu,nchunk,nvp)\n",
    "                _f = f.read('i_f', start=start, count=count).astype('float64')\n",
    "                lf.append(_f)\n",
    "        i_f = np.concatenate(lf, axis=2)\n",
    "        lb = np.concatenate(li)\n",
    "    elif fieldline is True:\n",
    "        import networkx as nx\n",
    "\n",
    "        fname2 = os.path.join(expdir, 'xgc.mesh.bp')\n",
    "        with ad2.open(fname2, 'r') as f:\n",
    "            _nnodes = int(f.read('n_n', ))\n",
    "            nextnode = f.read('nextnode')\n",
    "        \n",
    "        G = nx.Graph()\n",
    "        for i in range(_nnodes):\n",
    "            G.add_node(i)\n",
    "        for i in range(_nnodes):\n",
    "            G.add_edge(i, nextnode[i])\n",
    "            G.add_edge(nextnode[i], i)\n",
    "        cc = [x for x in list(nx.connected_components(G)) if len(x) >= 16]\n",
    "\n",
    "        li = list()\n",
    "        for k, components in enumerate(cc):\n",
    "            DG = nx.DiGraph()\n",
    "            for i in components:\n",
    "                DG.add_node(i)\n",
    "            for i in components:\n",
    "                DG.add_edge(i, nextnode[i])\n",
    "            \n",
    "            cycle = list(nx.find_cycle(DG))\n",
    "            DG.remove_edge(*cycle[-1])\n",
    "            \n",
    "            path = nx.dag_longest_path(DG)\n",
    "            #print (k, len(components), path[0])\n",
    "            for i in path[:len(path)-len(path)%16]:\n",
    "                li.append(i)\n",
    "\n",
    "        with ad2.open(fname, 'r') as f:\n",
    "            nstep, nsize = adios2_get_shape(f, 'i_f')\n",
    "            ndim = len(nsize)\n",
    "            nphi = nsize[0] if iphi is None else 1\n",
    "            iphi = 0 if iphi is None else iphi\n",
    "            _nnodes = nsize[2]\n",
    "            nmu = nsize[1]\n",
    "            nvp = nsize[3]\n",
    "            start = (iphi,0,0,0)\n",
    "            count = (nphi,nmu,_nnodes,nvp)\n",
    "            logging.info (f\"Reading: {start} {count}\")\n",
    "            i_f = f.read('i_f', start=start, count=count).astype('float64')\n",
    "        \n",
    "        _nnodes = len(li)-inode if nnodes is None else nnodes\n",
    "        lb = np.array(li[inode:inode+_nnodes], dtype=np.int32)\n",
    "        logging.info (f\"Fieldline: {len(lb)}\")\n",
    "        logging.info (f\"{lb}\")\n",
    "        i_f = i_f[:,:,lb,:]\n",
    "    else:\n",
    "        with ad2.open(fname, 'r') as f:\n",
    "            nstep, nsize = adios2_get_shape(f, 'i_f')\n",
    "            ndim = len(nsize)\n",
    "            nphi = nsize[0] if iphi is None else 1\n",
    "            iphi = 0 if iphi is None else iphi\n",
    "            _nnodes = nsize[2]-inode if nnodes is None else nnodes\n",
    "            nmu = nsize[1]\n",
    "            nvp = nsize[3]\n",
    "            start = (iphi,0,inode,0)\n",
    "            count = (nphi,nmu,_nnodes,nvp)\n",
    "            logging.info (f\"Reading: {start} {count}\")\n",
    "            i_f = f.read('i_f', start=start, count=count).astype('float64')\n",
    "            #e_f = f.read('e_f')\n",
    "        li = list(range(inode, inode+_nnodes))\n",
    "        lb = np.array(li, dtype=np.int32)\n",
    "\n",
    "    # if i_f.shape[3] == 31:\n",
    "    #     i_f = np.append(i_f, i_f[...,30:31], axis=3)\n",
    "    #     # e_f = np.append(e_f, e_f[...,30:31], axis=3)\n",
    "    if i_f.shape[3] == 39:\n",
    "        i_f = np.append(i_f, i_f[...,38:39], axis=3)\n",
    "        i_f = np.append(i_f, i_f[:,38:39,:,:], axis=1)\n",
    "\n",
    "    Z0 = np.moveaxis(i_f, 1, 2)\n",
    "\n",
    "    if average:\n",
    "        Z0 = np.mean(Z0, axis=0)\n",
    "        zlb = lb\n",
    "    else:\n",
    "        Z0 = Z0.reshape((-1,Z0.shape[2],Z0.shape[3]))\n",
    "        _lb = list()\n",
    "        for i in range(nphi):\n",
    "            _lb.append( i*100_000_000 + lb)\n",
    "        zlb = np.concatenate(_lb)\n",
    "    \n",
    "    #zlb = np.concatenate(li)\n",
    "    zmu = np.mean(Z0, axis=(1,2))\n",
    "    zsig = np.std(Z0, axis=(1,2))\n",
    "    zmin = np.min(Z0, axis=(1,2))\n",
    "    zmax = np.max(Z0, axis=(1,2))\n",
    "    Zif = (Z0 - zmin[:,np.newaxis,np.newaxis])/(zmax-zmin)[:,np.newaxis,np.newaxis]\n",
    "\n",
    "    return (Z0, Zif, zmu, zsig, zmin, zmax, zlb)\n",
    "\n",
    "def read_data(data_dir,num_channels=1):\n",
    "    Z0, Zif, zmu, zsig, zmin, zmax, zlb = read_f0(420, expdir=data_dir, iphi=0)\n",
    "    #print(Zif.shape, zlb.shape, zmu.shape, zsig.shape)\n",
    "    lx = list()\n",
    "    ly = list()\n",
    "    for i in range(0,len(Zif)-num_channels,num_channels):\n",
    "        X = Zif[i:i+num_channels,:,:]\n",
    "        lx.append(X)\n",
    "        ly.append(zlb[i:i+num_channels])\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(lx, ly, test_size=0.10, random_state=42)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "class XGCDataset:\n",
    "    def __init__(self, base_X, base_Y, split, transform=None, patch_size=5):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.split = split\n",
    "        self.image_list = base_X\n",
    "        self.label_list = base_Y\n",
    "        self.transform = transform\n",
    "        \n",
    "        orig_sz = base_X[0].shape[1]\n",
    "        self.image_size = int(orig_sz/patch_size)\n",
    "        \n",
    "        self.num_patches = int((orig_sz*orig_sz)/(patch_size*patch_size))\n",
    "        self.ids =[]\n",
    "        self.sub_ids=[]\n",
    "        \n",
    "        for i in range(0,len(self.image_list)):\n",
    "            self.ids+=self.num_patches*[i]\n",
    "            self.sub_ids+=range(0,self.num_patches)\n",
    "        \n",
    "        print('data init:',self.num_patches,len(self.image_list),len(self.ids),len(self.sub_ids))\n",
    "    \n",
    "    def __len__(self): \n",
    "        return len(self.ids)\n",
    "    \n",
    "    def __getitem__(self,i):\n",
    "        orig_image = self.image_list[self.ids[i]]\n",
    "        orig_image=orig_image[0,:,:]\n",
    "        #print(orig_image.shape)\n",
    "        \n",
    "        sub_idx = self.sub_ids[i]\n",
    "        \n",
    "        ridx = int(sub_idx/self.image_size)\n",
    "        cidx = int(sub_idx%self.image_size)\n",
    "        \n",
    "        rs= ridx*self.patch_size\n",
    "        re = rs+self.patch_size\n",
    "        cs = cidx*self.patch_size\n",
    "        ce = cs+self.patch_size\n",
    "        \n",
    "        image = orig_image[rs:re,cs:ce]\n",
    "        image = image[np.newaxis,:,:]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        sample = {'image': torch.as_tensor(image.copy()).float(), \n",
    "                  'label': self.label_list[self.ids[i]],\n",
    "                  'rsid': rs, 'csid':cs}\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64531a3",
   "metadata": {},
   "source": [
    "Hyper Network Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e33aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class HyperNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, f_size = 3, z_dim = 64, out_size=16, in_size=16):\n",
    "        super(HyperNetwork, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.f_size = f_size\n",
    "        self.out_size = out_size\n",
    "        self.in_size = in_size\n",
    "\n",
    "        self.w1 = Parameter(torch.fmod(torch.randn((self.z_dim, self.out_size*self.f_size*self.f_size)).cuda(),2))\n",
    "        self.b1 = Parameter(torch.fmod(torch.randn((self.out_size*self.f_size*self.f_size)).cuda(),2))\n",
    "\n",
    "        self.w2 = Parameter(torch.fmod(torch.randn((self.z_dim, self.in_size*self.z_dim)).cuda(),2))\n",
    "        self.b2 = Parameter(torch.fmod(torch.randn((self.in_size*self.z_dim)).cuda(),2))\n",
    "\n",
    "    def forward(self, z):\n",
    "\n",
    "        h_in = torch.matmul(z, self.w2) + self.b2\n",
    "        h_in = h_in.view(self.in_size, self.z_dim)\n",
    "\n",
    "        h_final = torch.matmul(h_in, self.w1) + self.b1\n",
    "        kernel = h_final.view(self.out_size, self.in_size, self.f_size, self.f_size)\n",
    "\n",
    "        return kernel\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dd3182",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ResNet Block\n",
    "\n",
    "class IdentityLayer(nn.Module):\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResNetBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_size=16, out_size=16, downsample = False):\n",
    "        super(ResNetBlock,self).__init__()\n",
    "        self.out_size = out_size\n",
    "        self.in_size = in_size\n",
    "        if downsample:\n",
    "            self.stride1 = 2\n",
    "            self.reslayer = nn.Conv2d(in_channels=self.in_size, out_channels=self.out_size, \n",
    "                                      stride=2, kernel_size=1)\n",
    "        else:\n",
    "            self.stride1 = 1\n",
    "            self.reslayer = IdentityLayer()\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(out_size)\n",
    "        self.bn2 = nn.BatchNorm2d(out_size)\n",
    "\n",
    "    def forward(self, x, conv1_w, conv2_w):\n",
    "\n",
    "        residual = self.reslayer(x)\n",
    "\n",
    "        out = F.relu(self.bn1(F.conv2d(x, conv1_w, stride=self.stride1, padding=1)), inplace=True)\n",
    "        out = self.bn2(F.conv2d(out, conv2_w, padding=1))\n",
    "\n",
    "        out += residual\n",
    "\n",
    "        out = F.relu(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9837c8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Embedding(nn.Module):\n",
    "\n",
    "    def __init__(self, z_num, z_dim):\n",
    "        super(Embedding, self).__init__()\n",
    "\n",
    "        self.z_list = nn.ParameterList()\n",
    "        self.z_num = z_num\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "        h,k = self.z_num\n",
    "\n",
    "        for i in range(h):\n",
    "            for j in range(k):\n",
    "                self.z_list.append(Parameter(torch.fmod(torch.randn(self.z_dim).cuda(), 2)))\n",
    "\n",
    "    def forward(self, hyper_net):\n",
    "        ww = []\n",
    "        h, k = self.z_num\n",
    "        for i in range(h):\n",
    "            w = []\n",
    "            for j in range(k):\n",
    "                w.append(hyper_net(self.z_list[i*k + j]))\n",
    "            ww.append(torch.cat(w, dim=1))\n",
    "        return torch.cat(ww, dim=0)\n",
    "\n",
    "\n",
    "class PrimaryNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, z_dim=64, patch_size=5):\n",
    "        super(PrimaryNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.patch_size= patch_size\n",
    "        self.z_dim = z_dim\n",
    "        self.hope = HyperNetwork(z_dim=self.z_dim)\n",
    "\n",
    "        self.zs_size = [[1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1],\n",
    "                        [2, 1], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2],\n",
    "                        [4, 2], [4, 4], [4, 4], [4, 4], [4, 4], [4, 4], [4, 4], [4, 4], [4, 4], [4, 4], [4, 4], [4, 4]]\n",
    "\n",
    "        self.filter_size = [[16,16], [16,16], [16,16], [16,16], [16,16], [16,16], [16,32], [32,32], [32,32], [32,32],\n",
    "                            [32,32], [32,32], [32,64], [64,64], [64,64], [64,64], [64,64], [64,64]]\n",
    "\n",
    "        self.res_net = nn.ModuleList()\n",
    "\n",
    "        for i in range(18):\n",
    "            down_sample = False\n",
    "            if i > 5 and i % 6 == 0:\n",
    "                down_sample = True\n",
    "            self.res_net.append(ResNetBlock(self.filter_size[i][0], self.filter_size[i][1], downsample=down_sample))\n",
    "\n",
    "        self.zs = nn.ModuleList()\n",
    "\n",
    "        for i in range(36):\n",
    "            self.zs.append(Embedding(self.zs_size[i], self.z_dim))\n",
    "\n",
    "        self.global_avg = nn.AvgPool2d(8)\n",
    "        self.final = nn.Linear(256,self.patch_size*self.patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        #print('conv1:',x.shape)\n",
    "        for i in range(18):\n",
    "            # if i != 15 and i != 17:\n",
    "            w1 = self.zs[2*i](self.hope)\n",
    "            w2 = self.zs[2*i+1](self.hope)\n",
    "            x = self.res_net[i](x, w1, w2)\n",
    "            #print('resnet:',i,x.shape)\n",
    "        \n",
    "        #print('final resnet:',x.shape)\n",
    "        \n",
    "        #x = self.global_avg(x)\n",
    "        #print('avg pool:',x.shape)\n",
    "        x = self.final(x.view(-1,256))\n",
    "        x = x.view(-1,1,self.patch_size,self.patch_size)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52d4781",
   "metadata": {},
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175f1f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11f43c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_data(patch_size, batch_size):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = read_data(dir_data)\n",
    "    trainset = XGCDataset(X_train, y_train, split=\"train\",\n",
    "                               transform=None, patch_size=patch_size)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=4)\n",
    "    \n",
    "    testset = XGCDataset(X_test, y_test, split=\"test\",\n",
    "                               transform=None, patch_size=patch_size)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=4)\n",
    "    \n",
    "    return trainloader, testloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab18a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(device, net, testloader, criterion, batch_size):\n",
    "    correct = 0.\n",
    "    total = len(testloader)*batch_size\n",
    "    \n",
    "    for tdata in testloader:\n",
    "        timages, tlabels = tdata['image'], tdata['label']\n",
    "        toutputs = net(Variable(timages.cuda()))\n",
    "        predicted = toutputs.cpu().data\n",
    "        correct+= criterion(predicted,timages)\n",
    "        \n",
    "    return correct.item(), total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0042e166",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 256\n",
    "save_freq = 20\n",
    "patch_size = 8 \n",
    "print_freq = 20\n",
    "dir_out = 'checkpoint/'\n",
    "dir_resume = 'checkpoint/hypernetworks_plasma.pth/'\n",
    "dir_data =  'gpfs/alpine/world-shared/csc143/jyc/summit/d3d_coarse_small_v2' \n",
    "resume = False\n",
    "\n",
    "\n",
    "if not os.path.exists(dir_out):\n",
    "        os.makedirs(dir_out)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518145e5",
   "metadata": {},
   "source": [
    "# Calculate Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a1ab0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = PrimaryNetwork(patch_size=patch_size)\n",
    "model_total_params = sum(p.numel() for p in net.parameters())\n",
    "print(model_total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80546557",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install pytorch-model-summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc02a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_model_summary import summary\n",
    "net = PrimaryNetwork(patch_size=8)\n",
    "total=0\n",
    "for name, param in net.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.numel())\n",
    "        if name.find('res_net'):\n",
    "            #print('resnet')\n",
    "            total+=param.numel()\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "\n",
    "print('total:',total)\n",
    "#trainloader, testloader = set_data(8, 256)\n",
    "#print(net)\n",
    "#for i, data in enumerate(trainloader, 0):\n",
    " #   inputs, labels = data['image'], data['label']\n",
    "    #inputs= Variable(inputs)\n",
    "#print(summary(net, inputs, show_input=True, print_summary=True))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca67e93",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233bccb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(device):\n",
    "    trainloader, testloader = set_data(patch_size, batch_size)\n",
    "    \n",
    "    net = PrimaryNetwork(patch_size=patch_size)\n",
    "    best_accuracy = 10000.\n",
    "\n",
    "    if resume:\n",
    "        ckpt = torch.load(args.dir_resume)\n",
    "        net.load_state_dict(ckpt['net'])\n",
    "        best_accuracy = ckpt['acc']\n",
    "\n",
    "    net.cuda()\n",
    "\n",
    "    learning_rate = 0.002\n",
    "    weight_decay = 0.0005\n",
    "    milestones = [168000, 336000, 400000, 450000, 550000, 600000]\n",
    "    max_iter = epochs\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=milestones, gamma=0.5)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    total_iter = 0\n",
    "    epoch = 0\n",
    "    #print_freq = args.print_freq\n",
    "    num_batches=len(trainloader)\n",
    "    loss_file=open(dir_out+'loss.txt','w')\n",
    "    loss_file.write('epoch,loss,val_loss\\n')\n",
    "    start = time.time()\n",
    "    \n",
    "    print('data loader:',len(trainloader),len(testloader))\n",
    "    while total_iter < max_iter:\n",
    "\n",
    "        running_loss = 0.0\n",
    "        epoch_loss=0\n",
    "        epoch=0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "            inputs, labels = data['image'], data['label']\n",
    "            rid, cid = data['rsid'], data['csid']\n",
    "\n",
    "            inputs= Variable(inputs.cuda())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            \n",
    "            outputs = net(inputs)\n",
    "            #print('outputs:',inputs.shape,outputs.shape)\n",
    "            \n",
    "            \n",
    "            loss = criterion(outputs, inputs)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            epoch_loss+=loss.item()\n",
    "            if i % print_freq == (print_freq-1):\n",
    "                print(\"[Epoch %d, Total Iterations %4d] Loss: %.4f\" % (epoch + 1, \n",
    "                                                                       total_iter + 1, \n",
    "                                                                       running_loss/print_freq))\n",
    "                running_loss = 0.0\n",
    "            \n",
    "            epoch += 1\n",
    "            \n",
    "        \n",
    "        total_iter += 1\n",
    "\n",
    "        correct, total= validate(device, net, testloader, criterion, batch_size)\n",
    "        epoch_loss/=num_batches\n",
    "        val_loss = correct/total\n",
    "        string1=str(total_iter)+','+str(epoch_loss)+','+str(val_loss)+'\\n'\n",
    "        loss_file.write(string1)\n",
    "        accuracy = (100. * correct) / total\n",
    "        print('After epoch %d, accuracy: %.4f %%' % (total_iter, accuracy))\n",
    "\n",
    "        if accuracy < best_accuracy:\n",
    "            print('Saving model...')\n",
    "            state = {\n",
    "                'net': net.state_dict(),\n",
    "                'acc': accuracy\n",
    "            }\n",
    "            torch.save(state, dir_out+'hypernetworks_plasma_'+str(total_iter)+'.pth')\n",
    "            best_accuracy = accuracy\n",
    "        \n",
    "    print('Finished Training')\n",
    "    state = {\n",
    "                'net': net.state_dict(),\n",
    "                'acc': accuracy\n",
    "            }\n",
    "    torch.save(state, dir_out+'last.pth')\n",
    "    loss_file.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e25476",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(device)\n",
    "\n",
    "print('Finished Training!!')   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375808e6",
   "metadata": {},
   "source": [
    "Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b9509d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(batch_size, dir_data, patch_size):\n",
    "    X_train, X_test, y_train, y_test = read_data(dir_data)\n",
    "    testset = XGCDataset(X_test, y_test, split=\"test\",\n",
    "                               transform=None, patch_size=patch_size)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=4)\n",
    "    \n",
    "    return testloader, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14505f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(device, dir_model):\n",
    "    net = PrimaryNetwork(patch_size=patch_size)\n",
    "    ckpt = torch.load(dir_model, map_location=device)\n",
    "    state_dict = ckpt['net']\n",
    "    \n",
    "    new_state_dict = {}\n",
    "    \n",
    "    for k, v in state_dict.items():\n",
    "        k = k.replace(\"module.\", \"\")\n",
    "        new_state_dict[k] = v\n",
    "        state_dict = new_state_dict\n",
    "    \n",
    "    #net.to(device=device)\n",
    "    net.load_state_dict(state_dict)\n",
    "    \n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545714dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(map_pred_img,predictions, rid, cid, labels, patch_size):\n",
    "    \n",
    "    for l in range(0,len(labels)):\n",
    "        map_pred_img[labels[l][0]][rid[l]:rid[l]+patch_size,cid[l]:cid[l]+patch_size] = predictions[l,:,:]\n",
    "        \n",
    "    return map_pred_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ca9379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5517ccf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_model_summary import summary\n",
    "dir_data = 'gpfs/alpine/world-shared/csc143/jyc/summit/d3d_coarse_small_v2'\n",
    "model = load_model(device, 'checkpoint/hypernetworks_plasma_51.pth')\n",
    "#testloader, Xtest, ytest = load_data(256, dir_data, 8)\n",
    "help(summary)\n",
    "#for i, data in enumerate(testloader,0):\n",
    " #       timages, tlabels = data['image'], data['label']\n",
    "print(summary(model, torch.zeros((1, 1, 8, 8)), show_input=False,print_summary=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45f5477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(device, batch_size, patch_size, dir_out, dir_model, dir_data):\n",
    "    model = load_model(device, dir_model)\n",
    "    testloader, Xtest, ytest = load_data(batch_size, dir_data, patch_size)\n",
    "    model.eval()\n",
    "    map_pred_img_ensemble={}\n",
    "    \n",
    "    for tid in ytest:\n",
    "        #print('test labels:',tid[0])\n",
    "        map_pred_img_ensemble[tid[0]]= np.zeros((40,40))\n",
    "    \n",
    "    print('testloader:',len(testloader))\n",
    "    \n",
    "    for i, data in enumerate(testloader,0):\n",
    "        timages, tlabels = data['image'], data['label']\n",
    "        rid, cid = data['rsid'], data['csid']\n",
    "        #timages = timages.to(device=device, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            toutputs = model(Variable(timages.cuda()))\n",
    "            predicted = toutputs.cpu().data\n",
    "        \n",
    "        predicted = predicted.squeeze()\n",
    "        predicted = predicted.numpy()\n",
    "        tlabels = list(tlabels.numpy())\n",
    "        rid = list(rid.numpy())\n",
    "        cid = list(cid.numpy())\n",
    "        orig_image = timages.numpy()\n",
    "        #print(i,predicted.shape,orig_image.shape,len(rid),len(cid),len(tlabels))\n",
    "        \n",
    "        map_pred_img_ensemble = aggregate(map_pred_img_ensemble,predicted, rid, cid, \n",
    "                                          tlabels, patch_size)\n",
    "    \n",
    "    fname = 'mse_xgc_testrmse_'+str(batch_size)+'_'+str(len(testloader))+'.txt'\n",
    "    error_file=open(dir_out+fname,'w')\n",
    "    total_err =0\n",
    "    X_pred =[]\n",
    "    for l in range(0,len(ytest)):\n",
    "        tid = ytest[l][0]\n",
    "        targets = map_pred_img_ensemble[tid]\n",
    "        #loss = np.mean((Xtest[l] - targets)**2)\n",
    "        loss = rmse(targets, Xtest[l][0])\n",
    "        total_err+=loss\n",
    "        #print(tid,loss,Xtest[l][0].shape, targets.shape)\n",
    "        string=str(tid)+','+str(loss.item())+'\\n'\n",
    "        error_file.write(string)\n",
    "        X_pred.append(targets)\n",
    "        \n",
    "    print('total:',total_err/(len(testloader)*batch_size))\n",
    "    string='total,'+str(total_err)+'\\n'\n",
    "    error_file.write(string)\n",
    "        \n",
    "    error_file.close()\n",
    "    \n",
    "    return Xtest, ytest, X_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ae4a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "patch_size = 8\n",
    "dir_out = 'results/'\n",
    "dir_model = 'checkpoint/hypernetworks_plasma_51.pth'\n",
    "dir_data = 'gpfs/alpine/world-shared/csc143/jyc/summit/d3d_coarse_small_v2'\n",
    "if not os.path.exists(dir_out):\n",
    "        os.makedirs(dir_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8019106",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "xtest, ytest, xpred = test(device,batch_size, patch_size, dir_out, dir_model, dir_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d184d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in range(0,len(ytest)):\n",
    "    np.save(dir_out+'f_pred_'+str(ytest[l][0]),xpred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07ea10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_, ny, nx = xtest[0].shape\n",
    "ix = np.linspace(0, nx-1, nx)\n",
    "iy = np.linspace(0, ny-1, ny)\n",
    "Mx, My = np.meshgrid(ix, iy)\n",
    "\n",
    "for l in range(0,len(ytest)):\n",
    "    plt.figure(figsize=(4, 9))\n",
    "    print('idx:',l)\n",
    "    f = plt.figure(figsize=(10, 6))\n",
    "    f.add_subplot(1,2, 1)\n",
    "    plt.imshow(xtest[l][0], origin='lower')\n",
    "    plt.colorbar()\n",
    "    plt.contour(Mx, My, xtest[l][0], levels=5, origin='image', colors='white', alpha=0.5)\n",
    "    plt.axis('scaled')\n",
    "    plt.axis('off')\n",
    "    plt.title('original %d'%(ytest[l][0]))\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    f.add_subplot(1,2, 2)\n",
    "    plt.imshow(xpred[l], origin='lower')\n",
    "    plt.colorbar()\n",
    "    plt.contour(Mx, My, xpred[l], levels=5, origin='image', colors='white', alpha=0.5)\n",
    "    plt.axis('scaled')\n",
    "    plt.axis('off')\n",
    "    plt.title('predicted %d'%(ytest[l][0]))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61bfa25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
