{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06ea107",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install adios2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0402efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import random \n",
    "import tqdm\n",
    "\n",
    "import adios2 as ad2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffc5aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_f0(istep, expdir=None, iphi=None, inode=0, nnodes=None, average=False, \n",
    "            randomread=0.0, nchunk=16, fieldline=False):\n",
    "    \"\"\"\n",
    "    Read XGC f0 data\n",
    "    \"\"\"\n",
    "    def adios2_get_shape(f, varname):\n",
    "        nstep = int(f.available_variables()[varname]['AvailableStepsCount'])\n",
    "        shape = f.available_variables()[varname]['Shape']\n",
    "        lshape = None\n",
    "        if shape == '':\n",
    "            ## Accessing Adios1 file\n",
    "            ## Read data and figure out\n",
    "            v = f.read(varname)\n",
    "            lshape = v.shape\n",
    "        else:\n",
    "            lshape = tuple([ int(x.strip(',')) for x in shape.strip().split() ])\n",
    "        return (nstep, lshape)\n",
    "\n",
    "    fname = os.path.join(expdir, 'restart_dir/xgc.f0.%05d.bp'%istep)\n",
    "    if randomread > 0.0:\n",
    "        ## prefetch to get metadata\n",
    "        with ad2.open(fname, 'r') as f:\n",
    "            nstep, nsize = adios2_get_shape(f, 'i_f')\n",
    "            ndim = len(nsize)\n",
    "            nphi = nsize[0]\n",
    "            _nnodes = nsize[2] if nnodes is None else nnodes\n",
    "            nmu = nsize[1]\n",
    "            nvp = nsize[3]\n",
    "        assert _nnodes%nchunk == 0\n",
    "        _lnodes = list(range(inode, inode+_nnodes, nchunk))\n",
    "        lnodes = random.sample(_lnodes, k=int(len(_lnodes)*randomread))\n",
    "        lnodes = np.sort(lnodes)\n",
    "\n",
    "        lf = list()\n",
    "        li = list()\n",
    "        for i in tqdm(lnodes):\n",
    "            li.append(np.array(range(i,i+nchunk), dtype=np.int32))\n",
    "            with ad2.open(fname, 'r') as f:\n",
    "                nphi = nsize[0] if iphi is None else 1\n",
    "                iphi = 0 if iphi is None else iphi\n",
    "                start = (iphi,0,i,0)\n",
    "                count = (nphi,nmu,nchunk,nvp)\n",
    "                _f = f.read('i_f', start=start, count=count).astype('float64')\n",
    "                lf.append(_f)\n",
    "        i_f = np.concatenate(lf, axis=2)\n",
    "        lb = np.concatenate(li)\n",
    "    elif fieldline is True:\n",
    "        import networkx as nx\n",
    "\n",
    "        fname2 = os.path.join(expdir, 'xgc.mesh.bp')\n",
    "        with ad2.open(fname2, 'r') as f:\n",
    "            _nnodes = int(f.read('n_n', ))\n",
    "            nextnode = f.read('nextnode')\n",
    "        \n",
    "        G = nx.Graph()\n",
    "        for i in range(_nnodes):\n",
    "            G.add_node(i)\n",
    "        for i in range(_nnodes):\n",
    "            G.add_edge(i, nextnode[i])\n",
    "            G.add_edge(nextnode[i], i)\n",
    "        cc = [x for x in list(nx.connected_components(G)) if len(x) >= 16]\n",
    "\n",
    "        li = list()\n",
    "        for k, components in enumerate(cc):\n",
    "            DG = nx.DiGraph()\n",
    "            for i in components:\n",
    "                DG.add_node(i)\n",
    "            for i in components:\n",
    "                DG.add_edge(i, nextnode[i])\n",
    "            \n",
    "            cycle = list(nx.find_cycle(DG))\n",
    "            DG.remove_edge(*cycle[-1])\n",
    "            \n",
    "            path = nx.dag_longest_path(DG)\n",
    "            #print (k, len(components), path[0])\n",
    "            for i in path[:len(path)-len(path)%16]:\n",
    "                li.append(i)\n",
    "\n",
    "        with ad2.open(fname, 'r') as f:\n",
    "            nstep, nsize = adios2_get_shape(f, 'i_f')\n",
    "            ndim = len(nsize)\n",
    "            nphi = nsize[0] if iphi is None else 1\n",
    "            iphi = 0 if iphi is None else iphi\n",
    "            _nnodes = nsize[2]\n",
    "            nmu = nsize[1]\n",
    "            nvp = nsize[3]\n",
    "            start = (iphi,0,0,0)\n",
    "            count = (nphi,nmu,_nnodes,nvp)\n",
    "            logging.info (f\"Reading: {start} {count}\")\n",
    "            i_f = f.read('i_f', start=start, count=count).astype('float64')\n",
    "        \n",
    "        _nnodes = len(li)-inode if nnodes is None else nnodes\n",
    "        lb = np.array(li[inode:inode+_nnodes], dtype=np.int32)\n",
    "        logging.info (f\"Fieldline: {len(lb)}\")\n",
    "        logging.info (f\"{lb}\")\n",
    "        i_f = i_f[:,:,lb,:]\n",
    "    else:\n",
    "        with ad2.open(fname, 'r') as f:\n",
    "            nstep, nsize = adios2_get_shape(f, 'i_f')\n",
    "            ndim = len(nsize)\n",
    "            nphi = nsize[0] if iphi is None else 1\n",
    "            iphi = 0 if iphi is None else iphi\n",
    "            _nnodes = nsize[2]-inode if nnodes is None else nnodes\n",
    "            nmu = nsize[1]\n",
    "            nvp = nsize[3]\n",
    "            start = (iphi,0,inode,0)\n",
    "            count = (nphi,nmu,_nnodes,nvp)\n",
    "            logging.info (f\"Reading: {start} {count}\")\n",
    "            i_f = f.read('i_f', start=start, count=count).astype('float64')\n",
    "            #e_f = f.read('e_f')\n",
    "        li = list(range(inode, inode+_nnodes))\n",
    "        lb = np.array(li, dtype=np.int32)\n",
    "\n",
    "    # if i_f.shape[3] == 31:\n",
    "    #     i_f = np.append(i_f, i_f[...,30:31], axis=3)\n",
    "    #     # e_f = np.append(e_f, e_f[...,30:31], axis=3)\n",
    "    if i_f.shape[3] == 39:\n",
    "        i_f = np.append(i_f, i_f[...,38:39], axis=3)\n",
    "        i_f = np.append(i_f, i_f[:,38:39,:,:], axis=1)\n",
    "\n",
    "    Z0 = np.moveaxis(i_f, 1, 2)\n",
    "\n",
    "    if average:\n",
    "        Z0 = np.mean(Z0, axis=0)\n",
    "        zlb = lb\n",
    "    else:\n",
    "        Z0 = Z0.reshape((-1,Z0.shape[2],Z0.shape[3]))\n",
    "        _lb = list()\n",
    "        for i in range(nphi):\n",
    "            _lb.append( i*100_000_000 + lb)\n",
    "        zlb = np.concatenate(_lb)\n",
    "    \n",
    "    #zlb = np.concatenate(li)\n",
    "    zmu = np.mean(Z0, axis=(1,2))\n",
    "    zsig = np.std(Z0, axis=(1,2))\n",
    "    zmin = np.min(Z0, axis=(1,2))\n",
    "    zmax = np.max(Z0, axis=(1,2))\n",
    "    Zif = (Z0 - zmin[:,np.newaxis,np.newaxis])/(zmax-zmin)[:,np.newaxis,np.newaxis]\n",
    "\n",
    "    return (Z0, Zif, zmu, zsig, zmin, zmax, zlb)\n",
    "\n",
    "def read_data(base_data_dir, super_data_dir, num_channels=1):\n",
    "    Z0, Zif, zmu, zsig, zmin, zmax, zlb = read_f0(420, expdir=base_data_dir, iphi=0)\n",
    "    Z0_s, Zif_s, zmu_s, zsig_s, zmin_s, zmax_s, zlb_s = read_f0(420, expdir=super_data_dir, iphi=0)\n",
    "    print('base:',Zif.shape, zlb.shape, zmu.shape, zsig.shape)\n",
    "    print('super:',Zif_s.shape, zlb_s.shape, zmu_s.shape, zsig_s.shape)\n",
    "    \n",
    "    lx = list()\n",
    "    ly = list()\n",
    "    for i in range(0,len(Zif)-num_channels,num_channels):\n",
    "        X = Zif[i:i+num_channels,:,:]\n",
    "        lx.append(X)\n",
    "        ly.append(zlb[i:i+num_channels])\n",
    "    \n",
    "    X_train, X_test, id_train, id_test = train_test_split(lx, ly, test_size=0.10, random_state=42)\n",
    "    \n",
    "    Y_train = list()\n",
    "    Y_test = list()\n",
    "    \n",
    "    for ids in id_train:\n",
    "        X= Zif_s[ids[0]:ids[0]+num_channels,:,:]\n",
    "        Y_train.append(X)\n",
    "    \n",
    "    for ids in id_test:\n",
    "        X= Zif_s[ids[0]:ids[0]+num_channels,:,:]\n",
    "        Y_test.append(X)\n",
    "    \n",
    "    \n",
    "    return X_train, X_test, Y_train, Y_test, id_train, id_test\n",
    "\n",
    "\n",
    "class XGCSuperDataset:\n",
    "    def __init__(self, base_X, base_Y, ids, transform=None, patch_size=5):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.image_list = base_X\n",
    "        self.label_list = base_Y\n",
    "        self.id_list = ids\n",
    "        self.transform = transform\n",
    "        \n",
    "        orig_sz = base_X[0].shape[1]\n",
    "        self.image_size = int(orig_sz/patch_size)\n",
    "        \n",
    "        self.num_patches = int((orig_sz*orig_sz)/(patch_size*patch_size))\n",
    "        self.ids =[]\n",
    "        self.sub_ids=[]\n",
    "        \n",
    "        for i in range(0,len(self.image_list)):\n",
    "            self.ids+=self.num_patches*[i]\n",
    "            self.sub_ids+=range(0,self.num_patches)\n",
    "        \n",
    "        print('data init:',self.num_patches,len(self.image_list),len(self.label_list),\n",
    "              len(self.ids),len(self.sub_ids))\n",
    "    \n",
    "    def __len__(self): \n",
    "        return len(self.ids)\n",
    "    \n",
    "    def __getitem__(self,i):\n",
    "        base_image = self.image_list[self.ids[i]]\n",
    "        base_image=base_image[0,:,:]\n",
    "        \n",
    "        super_image = self.label_list[self.ids[i]]\n",
    "        super_image=super_image[0,:,:]\n",
    "        #print(orig_image.shape)\n",
    "        \n",
    "        sub_idx = self.sub_ids[i]\n",
    "        \n",
    "        ridx = int(sub_idx/self.image_size)\n",
    "        cidx = int(sub_idx%self.image_size)\n",
    "        \n",
    "        rs= ridx*self.patch_size\n",
    "        re = rs+self.patch_size\n",
    "        cs = cidx*self.patch_size\n",
    "        ce = cs+self.patch_size\n",
    "        \n",
    "        image = base_image[rs:re,cs:ce]\n",
    "        image = image[np.newaxis,:,:]\n",
    "        \n",
    "        label = super_image[rs:re,cs:ce]\n",
    "        label = label[np.newaxis,:,:]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            label = self.transform(label)\n",
    "        \n",
    "        sample = {'X': torch.as_tensor(image.copy()).float(), \n",
    "                  'Y': torch.as_tensor(label.copy()).float(),\n",
    "                  'label': self.id_list[self.ids[i]],\n",
    "                  'rsid': rs, 'csid':cs}\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bb0b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ResNet Block\n",
    "\n",
    "class IdentityLayer(nn.Module):\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResNetBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_size=16, out_size=16, downsample = False):\n",
    "        super(ResNetBlock,self).__init__()\n",
    "        self.out_size = out_size\n",
    "        self.in_size = in_size\n",
    "        if downsample:\n",
    "            self.stride1 = 2\n",
    "            self.reslayer = nn.Conv2d(in_channels=self.in_size, out_channels=self.out_size, \n",
    "                                      stride=2, kernel_size=1)\n",
    "        else:\n",
    "            self.stride1 = 1\n",
    "            self.reslayer = IdentityLayer()\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(out_size)\n",
    "        self.bn2 = nn.BatchNorm2d(out_size)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(self.in_size, self.out_size, kernel_size=3, \n",
    "                               stride=self.stride1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(self.out_size, self.out_size, kernel_size=3, \n",
    "                               stride=1, padding=1)\n",
    "        \n",
    "\n",
    "    def forward(self, x): #, conv1_w, conv2_w\n",
    "\n",
    "        residual = self.reslayer(x)\n",
    "        #print('resnet block:',x.shape, residual.shape)\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        #print('out:',out.shape)\n",
    "        out += residual\n",
    "\n",
    "        out = F.relu(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79893e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrimaryNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, z_dim=64, patch_size=5):\n",
    "        super(PrimaryNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.patch_size= patch_size\n",
    "        '''\n",
    "        self.z_dim = z_dim\n",
    "        self.hope = HyperNetwork(z_dim=self.z_dim)\n",
    "        \n",
    "        self.zs_size = [[1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1],\n",
    "                        [2, 1], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2],\n",
    "                        [4, 2], [4, 4], [4, 4], [4, 4], [4, 4], [4, 4], [4, 4], [4, 4], [4, 4], [4, 4], [4, 4], [4, 4]]\n",
    "        '''\n",
    "        self.filter_size = [[16,16], [16,16], [16,16], [16,16], [16,16], [16,16], [16,32], [32,32], [32,32], [32,32],\n",
    "                            [32,32], [32,32], [32,64], [64,64], [64,64], [64,64], [64,64], [64,64]]\n",
    "\n",
    "        self.res_net = nn.ModuleList()\n",
    "\n",
    "        for i in range(18):\n",
    "            down_sample = False\n",
    "            if i > 5 and i % 6 == 0:\n",
    "                down_sample = True\n",
    "            self.res_net.append(ResNetBlock(self.filter_size[i][0], self.filter_size[i][1], downsample=down_sample))\n",
    "        '''\n",
    "        self.zs = nn.ModuleList()\n",
    "\n",
    "        for i in range(36):\n",
    "            self.zs.append(Embedding(self.zs_size[i], self.z_dim))\n",
    "        '''\n",
    "        self.global_avg = nn.AvgPool2d(8)\n",
    "        self.final = nn.Linear(256,self.patch_size*self.patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        #print('conv1:',x.shape)\n",
    "        for i in range(18):\n",
    "            # if i != 15 and i != 17:\n",
    "            #w1 = self.zs[2*i](self.hope)\n",
    "            #w2 = self.zs[2*i+1](self.hope)\n",
    "            x = self.res_net[i](x)\n",
    "            #print('resnet:',i,x.shape)\n",
    "        \n",
    "        #print('final resnet:',x.shape)\n",
    "        \n",
    "        #x = self.global_avg(x)\n",
    "        #print('avg pool:',x.shape)\n",
    "        x = self.final(x.view(-1,256))\n",
    "        x = x.view(-1,1,self.patch_size,self.patch_size)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b97cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ded9344",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_data(patch_size, batch_size):\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test, id_train, id_test = read_data(dir_base_data, dir_super_data)\n",
    "    \n",
    "    #print(len(X_train), len(X_test), len(Y_train), len(Y_test))\n",
    "   \n",
    "    trainset = XGCSuperDataset(X_train, Y_train, id_train, transform=None, patch_size=patch_size)\n",
    "    \n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=2)\n",
    "    \n",
    "    testset = XGCSuperDataset(X_test, Y_test, id_test, transform=None, patch_size=patch_size)\n",
    "    \n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=2)\n",
    "    \n",
    "    return trainloader, testloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6659b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(device, net, testloader, criterion, batch_size):\n",
    "    correct = 0.\n",
    "    total = len(testloader)*batch_size\n",
    "    \n",
    "    for tdata in testloader:\n",
    "        timages, tlabels = tdata['X'], tdata['Y']\n",
    "        tlables = Variable(tlabels.cuda())\n",
    "        toutputs = net(Variable(timages.cuda()))\n",
    "        predicted = toutputs.cpu().data\n",
    "        correct+= criterion(predicted,tlabels)\n",
    "        \n",
    "    return correct.item(), total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d52a789",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 256\n",
    "save_freq = 20\n",
    "patch_size = 8 \n",
    "print_freq = 20\n",
    "dir_out = 'checkpoint/resnet/'\n",
    "dir_resume = 'checkpoint/resnet/resnet_plasma.pth/'\n",
    "dir_base_data = '/gpfs/alpine/world-shared/csc143/jyc/summit/d3d_coarse_small_v2' \n",
    "dir_super_data = '/gpfs/alpine/world-shared/csc143/jyc/summit/d3d_coarse_small_v2_4x' \n",
    "resume = False\n",
    "\n",
    "\n",
    "if not os.path.exists(dir_out):\n",
    "        os.makedirs(dir_out)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95956be3",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a257404b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(device):\n",
    "    trainloader, testloader = set_data(patch_size, batch_size)\n",
    "    \n",
    "    net = PrimaryNetwork(patch_size=patch_size)\n",
    "    best_accuracy = 10000.\n",
    "\n",
    "    if resume:\n",
    "        ckpt = torch.load(args.dir_resume)\n",
    "        net.load_state_dict(ckpt['net'])\n",
    "        best_accuracy = ckpt['acc']\n",
    "\n",
    "    net.cuda()\n",
    "\n",
    "    learning_rate = 0.002\n",
    "    weight_decay = 0.0005\n",
    "    milestones = [168000, 336000, 400000, 450000, 550000, 600000]\n",
    "    max_iter = epochs\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=milestones, gamma=0.5)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    total_iter = 0\n",
    "    epoch = 0\n",
    "    #print_freq = args.print_freq\n",
    "    num_batches=len(trainloader)\n",
    "    loss_file=open(dir_out+'loss.txt','w')\n",
    "    loss_file.write('epoch,loss,val_loss\\n')\n",
    "    start = time.time()\n",
    "    \n",
    "    print('data loader:',len(trainloader),len(testloader))\n",
    "    while total_iter < max_iter:\n",
    "\n",
    "        running_loss = 0.0\n",
    "        epoch_loss=0\n",
    "        epoch=0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "            inputs, labels = data['X'], data['Y']\n",
    "            nid, rid, cid = data['label'],data['rsid'], data['csid']\n",
    "            \n",
    "            \n",
    "            inputs= Variable(inputs.cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            \n",
    "            outputs = net(inputs)\n",
    "            #print('train:',inputs.shape, labels.shape, outputs.shape)\n",
    "            \n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            epoch_loss+=loss.item()\n",
    "            if i % print_freq == (print_freq-1):\n",
    "                print(\"[Epoch %d, Total Iterations %4d] Loss: %.4f\" % (epoch + 1, \n",
    "                                                                       total_iter + 1, \n",
    "                                                                       running_loss/print_freq))\n",
    "                running_loss = 0.0\n",
    "            \n",
    "            epoch += 1\n",
    "        \n",
    "        total_iter += 1\n",
    "        \n",
    "        correct, total= validate(device, net, testloader, criterion, batch_size)\n",
    "        epoch_loss/=num_batches\n",
    "        val_loss = correct/total\n",
    "        string1=str(total_iter)+','+str(epoch_loss)+','+str(val_loss)+'\\n'\n",
    "        loss_file.write(string1)\n",
    "        accuracy = (100. * correct) / total\n",
    "        print('After epoch %d, accuracy: %.4f %%' % (total_iter, accuracy))\n",
    "\n",
    "        if accuracy < best_accuracy:\n",
    "            print('Saving model...')\n",
    "            state = {\n",
    "                'net': net.state_dict(),\n",
    "                'acc': accuracy\n",
    "            }\n",
    "            torch.save(state, dir_out+'hypernet_plasma_super_resolve_'+str(total_iter)+'.pth')\n",
    "            best_accuracy = accuracy\n",
    "        \n",
    "    print('Finished Training')\n",
    "    state = {\n",
    "                'net': net.state_dict(),\n",
    "                'acc': accuracy\n",
    "            }\n",
    "    torch.save(state, dir_out+'last.pth')\n",
    "    loss_file.close()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb60d8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(device)\n",
    "\n",
    "print('Finished Training!!')   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2f9851-7c6d-4f34-a8aa-0d6dbe28a993",
   "metadata": {},
   "source": [
    "Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48db8aad-ec6c-43fe-8af7-a0c3de05b0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(batch_size, dir_base_data, dir_super_data, patch_size):\n",
    "    X_train, X_test, Y_train, Y_test, id_train, id_test = read_data(dir_base_data, dir_super_data)\n",
    "    \n",
    "    testset = XGCSuperDataset(X_test, Y_test, id_test, transform=None, patch_size=patch_size)\n",
    "    \n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=4)\n",
    "    \n",
    "    return testloader, X_test, Y_test, id_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd0571d-8acc-41bf-868e-4a94a2ef350e",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = PrimaryNetwork(patch_size=patch_size)\n",
    "model_total_params = sum(p.numel() for p in net.parameters())\n",
    "print(model_total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d517981-f8f9-416b-bca9-f9a140d8dcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(device, dir_model):\n",
    "    net = PrimaryNetwork(patch_size=patch_size)\n",
    "    ckpt = torch.load(dir_model, map_location=device)\n",
    "    state_dict = ckpt['net']\n",
    "    \n",
    "    new_state_dict = {}\n",
    "    \n",
    "    for k, v in state_dict.items():\n",
    "        k = k.replace(\"module.\", \"\")\n",
    "        new_state_dict[k] = v\n",
    "        state_dict = new_state_dict\n",
    "    \n",
    "    net.to(device=device)\n",
    "    net.load_state_dict(state_dict)\n",
    "    \n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81afbda8-dc36-4205-967b-98cbe9c9bc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(map_pred_img,predictions, rid, cid, labels, patch_size):\n",
    "    \n",
    "    for l in range(0,len(labels)):\n",
    "        map_pred_img[labels[l][0]][rid[l]:rid[l]+patch_size,cid[l]:cid[l]+patch_size] = predictions[l,:,:]\n",
    "        \n",
    "    return map_pred_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c415d2cc-cbf3-4eb7-a774-ec3ece2dcbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce19a92-b44d-49ba-a2b1-b1ad68457225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(device, batch_size, patch_size, dir_out, dir_model, dir_base_data, dir_super_data):\n",
    "    model = load_model(device, dir_model)\n",
    "    \n",
    "    testloader, Xtest, Ytest, id_test = load_data(batch_size, dir_base_data, dir_super_data, patch_size)\n",
    "    model.eval()\n",
    "    map_pred_img_ensemble={}\n",
    "    \n",
    "    for tid in id_test:\n",
    "        #print('test labels:',tid[0])\n",
    "        map_pred_img_ensemble[tid[0]]= np.zeros((40,40))\n",
    "    \n",
    "    print('testloader:',len(testloader))\n",
    "    \n",
    "    for i, data in enumerate(testloader,0):\n",
    "        timages, tlabels = data['X'], data['Y']\n",
    "        nid, rid, cid = data['label'],data['rsid'], data['csid']\n",
    "        #timages = timages.to(device=device, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            toutputs = model(Variable(timages.cuda()))\n",
    "            predicted = toutputs.cpu().data\n",
    "        \n",
    "        predicted = predicted.squeeze()\n",
    "        predicted = predicted.numpy()\n",
    "        tids = list(nid.numpy())\n",
    "        rid = list(rid.numpy())\n",
    "        cid = list(cid.numpy())\n",
    "        #orig_image = tlabels.numpy()\n",
    "        #print(i,predicted.shape,orig_image.shape,len(rid),len(cid),len(tlabels))\n",
    "        \n",
    "        map_pred_img_ensemble = aggregate(map_pred_img_ensemble,predicted, rid, cid, \n",
    "                                          tids, patch_size)\n",
    "    \n",
    "    fname = 'rmse_xgc_test_'+str(batch_size)+'_'+str(len(testloader))+'.txt'\n",
    "    error_file=open(dir_out+fname,'w')\n",
    "    total_err =0\n",
    "    Ypred =[]\n",
    "    for l in range(0,len(id_test)):\n",
    "        tid = id_test[l][0]\n",
    "        targets = map_pred_img_ensemble[tid]\n",
    "        #loss = np.mean((Xtest[l] - targets)**2)\n",
    "        loss = rmse(targets, Ytest[l][0])\n",
    "        total_err+=loss\n",
    "        #print(tid,loss,Xtest[l][0].shape, targets.shape)\n",
    "        string=str(tid)+','+str(loss.item())+'\\n'\n",
    "        error_file.write(string)\n",
    "        Ypred.append(targets)\n",
    "        \n",
    "    print('total:',total_err/(len(testloader)*batch_size))\n",
    "    string='total,'+str(total_err)+'\\n'\n",
    "    error_file.write(string)\n",
    "        \n",
    "    error_file.close()\n",
    "    \n",
    "    return Xtest, Ytest, Ypred, id_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3aa533-c137-4a04-972e-d64ea6b0b57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "patch_size = 8\n",
    "dir_out = 'results/resnet/'\n",
    "dir_model = 'checkpoint/resnet/hypernet_plasma_super_resolve_99.pth'\n",
    "dir_base_data = '/gpfs/alpine/world-shared/csc143/jyc/summit/d3d_coarse_small_v2'\n",
    "dir_super_data = '/gpfs/alpine/world-shared/csc143/jyc/summit/d3d_coarse_small_v2_4x'\n",
    "if not os.path.exists(dir_out):\n",
    "        os.makedirs(dir_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c487ec3-d57a-405f-8985-d550dd85e0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "xtest, ytest, ypred, idtest = test(device,batch_size, patch_size, dir_out, dir_model,\n",
    "                                   dir_base_data, dir_super_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825b1ab4-1d4c-44b4-a09f-21f7eba1ea5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "result_dir= 'results/resnet/rmse_xgc_test_256_161.txt'\n",
    "\n",
    "rmse ={}\n",
    "with open(result_dir) as f:\n",
    "    for line in f:\n",
    "        node, rmse_sc = line.split(',')\n",
    "        if node!='total':\n",
    "            node = int(node)\n",
    "            rmse_sc = float(rmse_sc)\n",
    "            rmse[node] = rmse_sc\n",
    "\n",
    "            #print(node,rmse_sc)\n",
    "        \n",
    "        #num_lines+=1\n",
    "        \n",
    "sorted_rmse = dict(sorted(rmse.items(), key=lambda item: item[1]))\n",
    "\n",
    "node_ids = list(sorted_rmse.keys())\n",
    "print('best 10 rmse (lowest scores):')\n",
    "\n",
    "for nid in range(0,10):\n",
    "    print(node_ids[nid],sorted_rmse[node_ids[nid]])\n",
    "\n",
    "print('worst 10 rmse (highest scores):')\n",
    "\n",
    "for nid in range(1,11):\n",
    "    print(node_ids[-nid],sorted_rmse[node_ids[-nid]])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821a8a55-399e-4a46-9b90-61db01e4f867",
   "metadata": {},
   "source": [
    "Best Performing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700a6061-32fd-414f-af2f-0ebc17cc62b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_, ny, nx = ytest[0].shape\n",
    "ix = np.linspace(0, nx-1, nx)\n",
    "iy = np.linspace(0, ny-1, ny)\n",
    "Mx, My = np.meshgrid(ix, iy)\n",
    "\n",
    "best_k= [13223,14390,15324,15231,15863,15360,15635,15343,15373,15618]\n",
    "worst_k= [12245,13194,12305,11598,11984,11672,11974,13278,11670,11300]\n",
    "ytest_b = {}\n",
    "ytest_w={}\n",
    "\n",
    "for l in range(0,len(idtest)):\n",
    "    #print('ytest',idtest[l][0])\n",
    "    if idtest[l][0] in best_k:\n",
    "        ytest_b[idtest[l][0]] = l\n",
    "    \n",
    "    if idtest[l][0] in worst_k:\n",
    "        ytest_w[idtest[l][0]] = l\n",
    "    \n",
    "print('best performing model:')\n",
    "for l in range(0,len(best_k)):\n",
    "    #plt.figure(figsize=(4, 9))\n",
    "    print('idx:',l)\n",
    "    yid = ytest_b[best_k[l]]\n",
    "    f = plt.figure(figsize=(12, 8))\n",
    "    f.add_subplot(1,3, 1)\n",
    "    plt.imshow(xtest[yid][0], origin='lower')\n",
    "    #plt.colorbar()\n",
    "    plt.contour(Mx, My, ytest[yid][0], levels=5, origin='image', colors='white', alpha=0.5)\n",
    "    plt.axis('scaled')\n",
    "    plt.axis('off')\n",
    "    plt.title('input %d'%(best_k[l]))\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    f.add_subplot(1,3, 2)\n",
    "    plt.imshow(ytest[yid][0], origin='lower')\n",
    "    #plt.colorbar()\n",
    "    plt.contour(Mx, My, ytest[yid][0], levels=5, origin='image', colors='white', alpha=0.5)\n",
    "    plt.axis('scaled')\n",
    "    plt.axis('off')\n",
    "    plt.title('original %d'%(best_k[l]))\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    f.add_subplot(1,3, 3)\n",
    "    plt.imshow(ypred[yid], origin='lower')\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    plt.contour(Mx, My, ypred[yid], levels=5, origin='image', colors='white', alpha=0.5)\n",
    "    #RMSE = rmse(ypred[yid], ytest[yid][0])\n",
    "    #plt.text(.02,.95,'RMSE: {:.04f}'.format(RMSE), fontsize=14, c='white')\n",
    "    plt.axis('scaled')\n",
    "    plt.axis('off')\n",
    "    plt.title('predicted %d'%(best_k[l]))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0d7006-331d-4a23-bf9f-f6363c02d40c",
   "metadata": {},
   "source": [
    "Worst Performing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce20cc0-4fc0-4557-b4a7-b848d89f2bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('worst performing model:')\n",
    "for l in range(0,len(worst_k)):\n",
    "    #plt.figure(figsize=(4, 9))\n",
    "    print('idx:',l)\n",
    "    yid = ytest_w[worst_k[l]]\n",
    "    f = plt.figure(figsize=(12, 8))\n",
    "    f.add_subplot(1,3, 1)\n",
    "    plt.imshow(xtest[yid][0], origin='lower')\n",
    "    #plt.colorbar()\n",
    "    plt.contour(Mx, My, ytest[yid][0], levels=5, origin='image', colors='white', alpha=0.5)\n",
    "    plt.axis('scaled')\n",
    "    plt.axis('off')\n",
    "    plt.title('input %d'%(worst_k[l]))\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    f.add_subplot(1,3, 2)\n",
    "    plt.imshow(ytest[yid][0], origin='lower')\n",
    "    #plt.colorbar()\n",
    "    plt.contour(Mx, My, ytest[yid][0], levels=5, origin='image', colors='white', alpha=0.5)\n",
    "    plt.axis('scaled')\n",
    "    plt.axis('off')\n",
    "    plt.title('original %d'%(worst_k[l]))\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    f.add_subplot(1,3, 3)\n",
    "    plt.imshow(ypred[yid], origin='lower')\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    plt.contour(Mx, My, ypred[yid], levels=5, origin='image', colors='white', alpha=0.5)\n",
    "    #RMSE = rmse(ypred[l], ytest[yid][0])\n",
    "    #plt.text(.02,.95,'RMSE: {:.04f}'.format(RMSE), fontsize=14, c='white')\n",
    "    plt.axis('scaled')\n",
    "    plt.axis('off')\n",
    "    plt.title('predicted %d'%(worst_k[l]))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f20378-524f-41c9-b3c7-b3ada6556ed0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OLCF-CUDA11 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
