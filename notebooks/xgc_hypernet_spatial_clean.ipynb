{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5956f2-f071-45bb-8b1d-ab3774b43b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import random \n",
    "import tqdm\n",
    "\n",
    "import adios2 as ad2\n",
    "#import pymetis\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1b37d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip freeze | grep adios2\n",
    "conda install -c conda-forge pymetis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51af5791-9cd6-42b4-b36b-4b261bba7aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install adios2"
   ]
  },
  {
   "cell_type": "raw",
   "id": "439aa0e7-eba2-4a9d-b890-272f8ecb6bec",
   "metadata": {},
   "source": [
    "def spatial_dis(nnodes, adj_list, pos_r, pos_z, num_anchor=10):\n",
    "    \n",
    "    anchor_dis = np.zeros((num_anchor,num_anchor))\n",
    "    node_set_per_anchor={}\n",
    "    nearest_anchor_idx={}\n",
    "    \n",
    "    n_cuts, membership = pymetis.part_graph(num_anchor, adjacency=adj_list)\n",
    "    \n",
    "    anchor_r = np.zeros(num_anchor)\n",
    "    anchor_z = np.zeros(num_anchor)\n",
    "    \n",
    "    for i in range(num_anchor):\n",
    "        tmp = np.argwhere(np.array(membership) == i).ravel()\n",
    "        node_set_per_anchor[i] = tmp\n",
    "        for j in tmp:\n",
    "            nearest_anchor_idx[j] = i\n",
    "            anchor_r[i]+= pos_r[j]\n",
    "            anchor_z[i]+= pos_z[j]\n",
    "        \n",
    "        anchor_r[i]/=len(tmp)\n",
    "        anchor_z[i]/=len(tmp)\n",
    "    \n",
    "    for i in range(0, num_anchor):\n",
    "        for j in range(i+1, num_anchor): \n",
    "            anchor_dis[i,j] = np.sqrt((anchor_r[i]-anchor_r[j])**2 + (anchor_z[i]-anchor_z[j])**2)\n",
    "            anchor_dis[j,i] = np.sqrt((anchor_r[i]-anchor_r[j])**2 + (anchor_z[i]-anchor_z[j])**2)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    for i in range(0,nnodes):\n",
    "        min_dis = np.sqrt((pos_r[i]-pos_r[list_anchors[0]])**2 + (pos_z[i]-pos_z[list_anchors[0]])**2)\n",
    "        nearest_anc_pos = 0\n",
    "        for j in range(1, len(list_anchors)):\n",
    "            dis = np.sqrt((pos_r[i]-pos_r[list_anchors[j]])**2 + (pos_z[i]-pos_z[list_anchors[j]])**2)\n",
    "            if dis<min_dis:\n",
    "                min_dis = dis\n",
    "                nearest_anc_pos = j\n",
    "        \n",
    "        nearest_anchor_idx[i] = nearest_anc_pos  \n",
    "        if list_anchors[nearest_anc_pos] not in node_set_per_anchor:\n",
    "            node_set_per_anchor[list_anchors[nearest_anc_pos]]=[]\n",
    "        \n",
    "        node_set_per_anchor[list_anchors[nearest_anc_pos]].append(i)\n",
    "    \n",
    "    for key in node_set_per_anchor.keys():\n",
    "        print(key, len(node_set_per_anchor[key]))\n",
    "    '''\n",
    "    return anchor_dis, nearest_anchor_idx, node_set_per_anchor"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fd3f9e7c-8524-4844-b757-6092096d63df",
   "metadata": {},
   "source": [
    "def xgc_random_sample(num_samples, nnodes, list_nodes, adj_nodes):\n",
    "    l = random.sample(list_nodes, 1)\n",
    "    rem_nodes = list_nodes.copy()\n",
    "    sampled_node = l[0]\n",
    "    for i in range(1, num_samples):\n",
    "        rem_nodes.remove(sampled_node)\n",
    "        for j in range(nnodes):\n",
    "            if j!=sampled_node and adj_nodes[sampled_node][j]==1:\n",
    "                rem_nodes.remove(j)\n",
    "        \n",
    "        sampled_node = random.sample(rem_nodes, 1)\n",
    "        sampled_node=sampled_node[0]\n",
    "        l.append(sampled_node)\n",
    "    \n",
    "    return l\n",
    "\n",
    "\n",
    "len(list_anchors)\n",
    "with open('anchor_nodes.txt', 'w') as f:\n",
    "    for item in list_anchors:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "def select_anchor_nodes(nnodes, mesh_edges): # num_sample=33, core=9528, edge=14000\n",
    "    #pbound = nnodes-1\n",
    "    nodes_adj = np.zeros((nnodes,nnodes))\n",
    "    adj_list = list()\n",
    "    num_edge = mesh_edges.shape[1]\n",
    "    for i in range(0,num_edge):\n",
    "        list_nodes = mesh_edges[i,:]\n",
    "        nodes_adj[list_nodes[0]][list_nodes[1]] = 1\n",
    "        nodes_adj[list_nodes[0]][list_nodes[2]] = 1\n",
    "        nodes_adj[list_nodes[1]][list_nodes[2]] = 1\n",
    "        nodes_adj[list_nodes[1]][list_nodes[0]] = 1\n",
    "        nodes_adj[list_nodes[2]][list_nodes[0]] = 1\n",
    "        nodes_adj[list_nodes[2]][list_nodes[1]] = 1\n",
    "    \n",
    "    for i in range(nnodes):\n",
    "        tmp = []\n",
    "        for j in range(nnodes):\n",
    "            if i!=j and nodes_adj[i][j]:\n",
    "                tmp.append(j)\n",
    "        adj_list.append(np.array(tmp))\n",
    "    '''\n",
    "    core_nodes = np.arange(0, core+1, dtype=int)\n",
    "    edge_nodes = np.arange(core+1, edge+1, dtype=int)\n",
    "    pbound_nodes = np.arange(edge+1, nnodes, dtype=int)\n",
    "    \n",
    "    sample_core = xgc_random_sample(num_sample, nnodes, list(core_nodes), nodes_adj)\n",
    "    sample_edge = xgc_random_sample(num_sample, nnodes, list(edge_nodes), nodes_adj)\n",
    "    sample_pbound = xgc_random_sample(num_sample, nnodes, list(pbound_nodes), nodes_adj)\n",
    "    \n",
    "    list_anchors = sample_core+sample_edge\n",
    "    list_anchors += sample_pbound\n",
    "    \n",
    "    return list_anchors, sample_core, sample_edge, sample_pbound\n",
    "    '''\n",
    "    return adj_lis\n",
    "\n",
    "#list_anchors, sample_core, sample_edge, sample_pb = select_anchor_nodes(nnodes, conn)\n",
    "nodes_adj = select_anchor_nodes(nnodes, conn)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e2ae1702-38fa-4601-b6af-57178316fe71",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.tri as tri\n",
    "\n",
    "plt.figure(figsize=[8,16])\n",
    "\n",
    "trimesh = tri.Triangulation(r, z, conn)\n",
    "plt.triplot(trimesh, alpha=0.3)\n",
    "plt.xlabel('R[m]')\n",
    "plt.ylabel('Z[m]')\n",
    "plt.title('XGC mesh')\n",
    "plt.axis('scaled')\n",
    "plt.tight_layout()\n",
    "for i in list_anchors:\n",
    "  plt.scatter(r[i], z[i], c='r', marker='x', s=40)\n",
    "  plt.text(r[i], z[i], i)\n",
    "\n",
    "plt.savefig('anchor-plot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c5e6fa-5b48-4e20-8d4c-b3c12bcb3156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_f0(istep, expdir=None, iphi=None, inode=0, nnodes=None, average=False, \n",
    "            randomread=0.0, nchunk=16, fieldline=False):\n",
    "    \"\"\"\n",
    "    Read XGC f0 data\n",
    "    \"\"\"\n",
    "    def adios2_get_shape(f, varname):\n",
    "        nstep = int(f.available_variables()[varname]['AvailableStepsCount'])\n",
    "        shape = f.available_variables()[varname]['Shape']\n",
    "        lshape = None\n",
    "        if shape == '':\n",
    "            ## Accessing Adios1 file\n",
    "            ## Read data and figure out\n",
    "            v = f.read(varname)\n",
    "            lshape = v.shape\n",
    "        else:\n",
    "            lshape = tuple([ int(x.strip(',')) for x in shape.strip().split() ])\n",
    "        return (nstep, lshape)\n",
    "\n",
    "    fname = os.path.join(expdir, 'restart_dir/xgc.f0.%05d.bp'%istep)\n",
    "    if randomread > 0.0:\n",
    "        ## prefetch to get metadata\n",
    "        with ad2.open(fname, 'r') as f:\n",
    "            nstep, nsize = adios2_get_shape(f, 'i_f')\n",
    "            ndim = len(nsize)\n",
    "            nphi = nsize[0]\n",
    "            _nnodes = nsize[2] if nnodes is None else nnodes\n",
    "            nmu = nsize[1]\n",
    "            nvp = nsize[3]\n",
    "        assert _nnodes%nchunk == 0\n",
    "        _lnodes = list(range(inode, inode+_nnodes, nchunk))\n",
    "        lnodes = random.sample(_lnodes, k=int(len(_lnodes)*randomread))\n",
    "        lnodes = np.sort(lnodes)\n",
    "\n",
    "        lf = list()\n",
    "        li = list()\n",
    "        for i in tqdm(lnodes):\n",
    "            li.append(np.array(range(i,i+nchunk), dtype=np.int32))\n",
    "            with ad2.open(fname, 'r') as f:\n",
    "                nphi = nsize[0] if iphi is None else 1\n",
    "                iphi = 0 if iphi is None else iphi\n",
    "                start = (iphi,0,i,0)\n",
    "                count = (nphi,nmu,nchunk,nvp)\n",
    "                _f = f.read('i_f', start=start, count=count).astype('float64')\n",
    "                lf.append(_f)\n",
    "        i_f = np.concatenate(lf, axis=2)\n",
    "        lb = np.concatenate(li)\n",
    "    elif fieldline is True:\n",
    "        import networkx as nx\n",
    "\n",
    "        fname2 = os.path.join(expdir, 'xgc.mesh.bp')\n",
    "        with ad2.open(fname2, 'r') as f:\n",
    "            _nnodes = int(f.read('n_n', ))\n",
    "            nextnode = f.read('nextnode')\n",
    "        \n",
    "        G = nx.Graph()\n",
    "        for i in range(_nnodes):\n",
    "            G.add_node(i)\n",
    "        for i in range(_nnodes):\n",
    "            G.add_edge(i, nextnode[i])\n",
    "            G.add_edge(nextnode[i], i)\n",
    "        cc = [x for x in list(nx.connected_components(G)) if len(x) >= 16]\n",
    "\n",
    "        li = list()\n",
    "        for k, components in enumerate(cc):\n",
    "            DG = nx.DiGraph()\n",
    "            for i in components:\n",
    "                DG.add_node(i)\n",
    "            for i in components:\n",
    "                DG.add_edge(i, nextnode[i])\n",
    "            \n",
    "            cycle = list(nx.find_cycle(DG))\n",
    "            DG.remove_edge(*cycle[-1])\n",
    "            \n",
    "            path = nx.dag_longest_path(DG)\n",
    "            #print (k, len(components), path[0])\n",
    "            for i in path[:len(path)-len(path)%16]:\n",
    "                li.append(i)\n",
    "\n",
    "        with ad2.open(fname, 'r') as f:\n",
    "            nstep, nsize = adios2_get_shape(f, 'i_f')\n",
    "            ndim = len(nsize)\n",
    "            nphi = nsize[0] if iphi is None else 1\n",
    "            iphi = 0 if iphi is None else iphi\n",
    "            _nnodes = nsize[2]\n",
    "            nmu = nsize[1]\n",
    "            nvp = nsize[3]\n",
    "            start = (iphi,0,0,0)\n",
    "            count = (nphi,nmu,_nnodes,nvp)\n",
    "            logging.info (f\"Reading: {start} {count}\")\n",
    "            i_f = f.read('i_f', start=start, count=count).astype('float64')\n",
    "        \n",
    "        _nnodes = len(li)-inode if nnodes is None else nnodes\n",
    "        lb = np.array(li[inode:inode+_nnodes], dtype=np.int32)\n",
    "        logging.info (f\"Fieldline: {len(lb)}\")\n",
    "        logging.info (f\"{lb}\")\n",
    "        i_f = i_f[:,:,lb,:]\n",
    "    else:\n",
    "        with ad2.open(fname, 'r') as f:\n",
    "            nstep, nsize = adios2_get_shape(f, 'i_f')\n",
    "            ndim = len(nsize)\n",
    "            nphi = nsize[0] if iphi is None else 1\n",
    "            iphi = 0 if iphi is None else iphi\n",
    "            _nnodes = nsize[2]-inode if nnodes is None else nnodes\n",
    "            nmu = nsize[1]\n",
    "            nvp = nsize[3]\n",
    "            start = (iphi,0,inode,0)\n",
    "            count = (nphi,nmu,_nnodes,nvp)\n",
    "            logging.info (f\"Reading: {start} {count}\")\n",
    "            i_f = f.read('i_f', start=start, count=count).astype('float64')\n",
    "            #e_f = f.read('e_f')\n",
    "        li = list(range(inode, inode+_nnodes))\n",
    "        lb = np.array(li, dtype=np.int32)\n",
    "\n",
    "    if i_f.shape[3] == 39:\n",
    "        i_f = np.append(i_f, i_f[...,38:39], axis=3)\n",
    "        i_f = np.append(i_f, i_f[:,38:39,:,:], axis=1)\n",
    "\n",
    "    Z0 = np.moveaxis(i_f, 1, 2)\n",
    "\n",
    "    if average:\n",
    "        Z0 = np.mean(Z0, axis=0)\n",
    "        zlb = lb\n",
    "    else:\n",
    "        Z0 = Z0.reshape((-1,Z0.shape[2],Z0.shape[3]))\n",
    "        _lb = list()\n",
    "        for i in range(nphi):\n",
    "            _lb.append( i*100_000_000 + lb)\n",
    "        zlb = np.concatenate(_lb)\n",
    "    \n",
    "    #zlb = np.concatenate(li)\n",
    "    zmu = np.mean(Z0, axis=(1,2))\n",
    "    zsig = np.std(Z0, axis=(1,2))\n",
    "    zmin = np.min(Z0, axis=(1,2))\n",
    "    zmax = np.max(Z0, axis=(1,2))\n",
    "    Zif = (Z0 - zmin[:,np.newaxis,np.newaxis])/(zmax-zmin)[:,np.newaxis,np.newaxis]\n",
    "\n",
    "    return (Z0, Zif, zmu, zsig, zmin, zmax, zlb)\n",
    "\n",
    "def read_data(base_data_dir, super_data_dir, num_channels=1):\n",
    "    Z0, Zif, zmu, zsig, zmin, zmax, zlb = read_f0(420, expdir=base_data_dir, iphi=0)\n",
    "    Z0_s, Zif_s, zmu_s, zsig_s, zmin_s, zmax_s, zlb_s = read_f0(420, expdir=super_data_dir, iphi=0)\n",
    "    #print('base:',Zif.shape, zlb.shape, zmu.shape, zsig.shape)\n",
    "    #print('super:',Zif_s.shape, zlb_s.shape, zmu_s.shape, zsig_s.shape)\n",
    "    \n",
    "    lx = list()\n",
    "    ly = list()\n",
    "    for i in range(0,len(Zif)-num_channels,num_channels):\n",
    "        X = Zif[i:i+num_channels,:,:]\n",
    "        lx.append(X)\n",
    "        ly.append(zlb[i:i+num_channels])\n",
    "    \n",
    "    X_train, X_test, id_train, id_test = train_test_split(lx, ly, test_size=0.10, random_state=42)\n",
    "    \n",
    "    #Y_train = list()\n",
    "    #Y_test = list()\n",
    "    x_s = list()\n",
    "    for i in range(0,len(Zif_s)-num_channels,num_channels):\n",
    "        X = Zif_s[i:i+num_channels,:,:]\n",
    "        x_s.append(X)\n",
    "    \n",
    "    '''\n",
    "    for ids in id_train:\n",
    "        X= Zif_s[ids[0]:ids[0]+num_channels,:,:]\n",
    "        Y_train.append(X)\n",
    "    \n",
    "    for ids in id_test:\n",
    "        X= Zif_s[ids[0]:ids[0]+num_channels,:,:]\n",
    "        Y_test.append(X)\n",
    "    '''\n",
    "    \n",
    "    return lx,x_s, id_train, id_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b71d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGCSuperDataset:\n",
    "    def __init__(self, base_X, base_Y, ids, num_cluster, cluster_per_node,\n",
    "                 transform=None, patch_size=5):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.image_list = base_X\n",
    "        self.label_list = base_Y\n",
    "        #self.spatial_si = spatial_pos\n",
    "        self.id_list = ids\n",
    "        self.transform = transform\n",
    "        self.cluster_per_node = cluster_per_node\n",
    "        self.num_cluster = num_cluster\n",
    "        \n",
    "        orig_sz = base_X[0].shape[1]\n",
    "        self.image_size = int(orig_sz/patch_size)\n",
    "        \n",
    "        self.num_patches = int((orig_sz*orig_sz)/(patch_size*patch_size))\n",
    "        self.ids =[]\n",
    "        self.sub_ids=[]\n",
    "        \n",
    "        for i in range(0,len(self.id_list)):\n",
    "            self.ids+=self.num_patches*[self.id_list[i]]\n",
    "            self.sub_ids+=range(0,self.num_patches)\n",
    "        \n",
    "        #print('data init:',len(self.image_list),len(self.label_list),len(self.ids),len(self.sub_ids),np.max(self.ids))\n",
    "    \n",
    "    def __len__(self): \n",
    "        return len(self.ids)\n",
    "    \n",
    "    def __getitem__(self,i):\n",
    "        base_image = self.image_list[self.ids[i]]\n",
    "        base_image=base_image[0,:,:]\n",
    "        \n",
    "        super_image = self.label_list[self.ids[i]]\n",
    "        super_image=super_image[0,:,:]\n",
    "        #print(orig_image.shape)\n",
    "        \n",
    "        sub_idx = self.sub_ids[i]\n",
    "        \n",
    "        ridx = int(sub_idx/self.image_size)\n",
    "        cidx = int(sub_idx%self.image_size)\n",
    "        \n",
    "        rs= ridx*self.patch_size\n",
    "        re = rs+self.patch_size\n",
    "        cs = cidx*self.patch_size\n",
    "        ce = cs+self.patch_size\n",
    "        \n",
    "        image = base_image[rs:re,cs:ce]\n",
    "        image = image[np.newaxis,:,:]\n",
    "        \n",
    "        label = super_image[rs:re,cs:ce]\n",
    "        label = label[np.newaxis,:,:]\n",
    "        \n",
    "        cluster_id = self.cluster_per_node[self.ids[i]]\n",
    "        si = np.zeros(self.num_cluster)\n",
    "        si[cluster_id] = 1 \n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            label = self.transform(label)\n",
    "        \n",
    "        sample = {'X': torch.as_tensor(image.copy()).float(), \n",
    "                  'Y': torch.as_tensor(label.copy()),\n",
    "                  'S': torch.as_tensor(si.copy()),\n",
    "                  'label': self.ids[i],\n",
    "                  'rsid': rs, 'csid':cs}\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184679bc",
   "metadata": {},
   "source": [
    "# Select anchor nodes over mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389abf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"/Users/oit/desktop/anika-macbook/datasets/ornl/fusion-energy-xgc/d3d_coarse_v2_colab\"\n",
    "inputdir=\"/gpfs/alpine/world-shared/csc143/jyc/summit\"\n",
    "with ad2.open('{}/d3d_coarse_small_v2/xgc.mesh.bp'.format(inputdir), 'r') as f:\n",
    "    nnodes = int(f.read('n_n', ))\n",
    "    ncells = int(f.read('n_t', ))\n",
    "    rz = f.read('rz')\n",
    "    conn = f.read('nd_connect_list')\n",
    "    psi = f.read('psi')\n",
    "    \n",
    "r = rz[:,0]\n",
    "z = rz[:,1]\n",
    "sml_nphi = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e90b970",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.shape,psi.shape,r.shape,z.shape,nnodes,ncells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6530201-2ce2-4a83-8afc-4dd15f3629c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_cluster(nnodes,r,z,num_anchor=5):\n",
    "    pairwise_dis=np.zeros(nnodes)\n",
    "    #vec_0 = np.sqrt((r[0]**2+z[0]**2))\n",
    "    \n",
    "    for i in range(1, nnodes):\n",
    "        p = (r[i]-r[0])**2 + (z[i]-z[0])**2\n",
    "        pairwise_dis[i] = np.sqrt(p)\n",
    "    \n",
    "    #print(np.min(pairwise_dis), np.max(pairwise_dis), np.mean(pairwise_dis[1:]))\n",
    "    k = num_anchor\n",
    "    nodes_per_cluster = int(nnodes/k)\n",
    "    node_set_per_anchor={}\n",
    "    cluster_per_node={}\n",
    "    \n",
    "    for i in range(k):\n",
    "        node_set_per_anchor[i]=[]\n",
    "    \n",
    "    for i in range(nnodes):\n",
    "        if i<=nodes_per_cluster:\n",
    "            cluster_per_node[i] =0\n",
    "            node_set_per_anchor[0].append(i)\n",
    "        elif i>=nodes_per_cluster and i<=2*nodes_per_cluster:\n",
    "            cluster_per_node[i] =1\n",
    "            node_set_per_anchor[1].append(i)\n",
    "        elif i>2*nodes_per_cluster and i<= 3*nodes_per_cluster:\n",
    "            cluster_per_node[i] =2\n",
    "            node_set_per_anchor[2].append(i)\n",
    "        elif i>3*nodes_per_cluster and i<=4*nodes_per_cluster:\n",
    "            cluster_per_node[i] =3\n",
    "            node_set_per_anchor[3].append(i)\n",
    "        elif i>4*nodes_per_cluster:\n",
    "            cluster_per_node[i] =4\n",
    "            node_set_per_anchor[4].append(i)\n",
    "\n",
    "\n",
    "    return pairwise_dis, cluster_per_node, node_set_per_anchor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96897bba-3174-449d-9aa0-ac081a4d233d",
   "metadata": {},
   "source": [
    "Hyper Network Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90926f1-eaf4-4839-8725-34bd18bf245c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class HyperNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, f_size = 3, z_dim = 64, out_size=16, in_size=16,s_dim=100):\n",
    "        super(HyperNetwork, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.f_size = f_size\n",
    "        self.out_size = out_size\n",
    "        self.in_size = in_size\n",
    "        self.s_dim = s_dim\n",
    "        \n",
    "        #64 X 16*3*3\n",
    "        self.w1 = Parameter(torch.fmod(torch.randn((self.z_dim, self.out_size*self.f_size*self.f_size)).cuda(),2))\n",
    "        #16*3*3\n",
    "        self.b1 = Parameter(torch.fmod(torch.randn((self.out_size*self.f_size*self.f_size)).cuda(),2))\n",
    "        \n",
    "        #64 X 16*64\n",
    "        self.w2 = Parameter(torch.fmod(torch.randn((self.z_dim, self.in_size*self.z_dim)).cuda(),2))\n",
    "        #16*64 X 1\n",
    "        self.b2 = Parameter(torch.fmod(torch.randn((self.in_size*self.z_dim)).cuda(),2))\n",
    "        \n",
    "        #100 X 16*64\n",
    "        self.w3 = Parameter(torch.fmod(torch.randn((self.s_dim, self.in_size*self.z_dim)).cuda(),2))\n",
    "        #16*64 X 1\n",
    "        self.b3 = Parameter(torch.fmod(torch.randn((self.in_size*self.z_dim)).cuda(),2))\n",
    "        \n",
    "    def forward(self, z,si): #100 \n",
    "\n",
    "        h_in = torch.matmul(z, self.w2) + self.b2    # input 64 dim , output = 16*64\n",
    "        h_in = h_in.view(self.in_size, self.z_dim)   #16 X 64\n",
    "        \n",
    "        h_spatial_in = torch.matmul(si,self.w3) + self.b3  #input 100 dim, output = 16*64 \n",
    "        h_spatial_in = h_spatial_in.view(self.in_size, self.z_dim) #16 X 64\n",
    "        \n",
    "        h_new = h_spatial_in + h_in #Add values of learnable and spatial embeddings once in same latent space\n",
    "\n",
    "        #h_final = torch.matmul(h_in, self.w1) + self.b1  # input 16 X 64, output = 16 X 16*3*3 \n",
    "        #kernel = h_final.view(self.out_size, self.in_size, self.f_size, self.f_size) # 16 X 16 X 3 X 3\n",
    "        \n",
    "        h_final = torch.matmul(h_new, self.w1) + self.b1  # input 16 X 64, output = 16 X 16*3*3 \n",
    "        kernel = h_final.view(self.out_size, self.in_size, self.f_size, self.f_size) # 16 X 16 X 3 X 3\n",
    "\n",
    "        return kernel\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa2c45f-0bab-42b3-8009-a6b9c66bf78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ResNet Block\n",
    "\n",
    "class IdentityLayer(nn.Module):\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResNetBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_size=16, out_size=16, downsample = False):\n",
    "        super(ResNetBlock,self).__init__()\n",
    "        self.out_size = out_size\n",
    "        self.in_size = in_size\n",
    "        if downsample:\n",
    "            self.stride1 = 2\n",
    "            self.reslayer = nn.Conv2d(in_channels=self.in_size, out_channels=self.out_size, \n",
    "                                      stride=2, kernel_size=1)\n",
    "        else:\n",
    "            self.stride1 = 1\n",
    "            self.reslayer = IdentityLayer()\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(out_size)\n",
    "        self.bn2 = nn.BatchNorm2d(out_size)\n",
    "\n",
    "    def forward(self, x, conv1_w, conv2_w):\n",
    "\n",
    "        residual = self.reslayer(x)\n",
    "\n",
    "        out = F.relu(self.bn1(F.conv2d(x, conv1_w, stride=self.stride1, padding=1)), inplace=True)\n",
    "        out = self.bn2(F.conv2d(out, conv2_w, padding=1))\n",
    "\n",
    "        out += residual\n",
    "\n",
    "        out = F.relu(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d50809a-9946-493a-a5e5-e496578a6f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Embedding(nn.Module):\n",
    "\n",
    "    def __init__(self, z_num, z_dim):\n",
    "        super(Embedding, self).__init__()\n",
    "\n",
    "        self.z_list = nn.ParameterList()\n",
    "        self.z_num = z_num\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "        h,k = self.z_num\n",
    "\n",
    "        for i in range(h):\n",
    "            for j in range(k):\n",
    "                self.z_list.append(Parameter(torch.fmod(torch.randn(self.z_dim).cuda(), 2)))\n",
    "\n",
    "    def forward(self, hyper_net,si): #si is the spatial encoding vector.\n",
    "        ww = []\n",
    "        h, k = self.z_num\n",
    "        for i in range(h):\n",
    "            w = []\n",
    "            for j in range(k):\n",
    "                w.append(hyper_net(self.z_list[i*k + j],si))\n",
    "            ww.append(torch.cat(w, dim=1))\n",
    "        return torch.cat(ww, dim=0)\n",
    "\n",
    "\n",
    "class PrimaryNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, z_dim=64, patch_size=5, s_dim=99):\n",
    "        super(PrimaryNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.patch_size= patch_size\n",
    "        self.z_dim = z_dim\n",
    "        self.s_dim = s_dim\n",
    "        self.hope = HyperNetwork(z_dim=self.z_dim, s_dim = self.s_dim)\n",
    "\n",
    "        self.zs_size = [[1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1],\n",
    "                        [2, 1], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2],\n",
    "                        [4, 2], [4, 4], [4, 4], [4, 4], [4, 4], [4, 4], [4, 4], [4, 4], [4, 4], [4, 4], [4, 4], [4, 4]]\n",
    "\n",
    "        self.filter_size = [[16,16], [16,16], [16,16], [16,16], [16,16], [16,16], [16,32], [32,32], [32,32], [32,32],\n",
    "                            [32,32], [32,32], [32,64], [64,64], [64,64], [64,64], [64,64], [64,64]]\n",
    "\n",
    "        self.res_net = nn.ModuleList()\n",
    "\n",
    "        for i in range(18):\n",
    "            down_sample = False\n",
    "            if i > 5 and i % 6 == 0:\n",
    "                down_sample = True\n",
    "            self.res_net.append(ResNetBlock(self.filter_size[i][0], self.filter_size[i][1], downsample=down_sample))\n",
    "\n",
    "        self.zs = nn.ModuleList()\n",
    "\n",
    "        for i in range(36):\n",
    "            self.zs.append(Embedding(self.zs_size[i], self.z_dim))\n",
    "\n",
    "        self.global_avg = nn.AvgPool2d(8)\n",
    "        self.final = nn.Linear(256,self.patch_size*self.patch_size)\n",
    "\n",
    "    def forward(self, x,si): # where si is the (num_nodes X 1) spatial-encoding vector\n",
    "\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        #print('conv1:',x.shape)\n",
    "        for i in range(18):\n",
    "            # if i != 15 and i != 17:\n",
    "            w1 = self.zs[2*i](self.hope,si) #Note the SAME si is passed for all sub_parts x of a single image X_i\n",
    "            w2 = self.zs[2*i+1](self.hope,si)\n",
    "            x = self.res_net[i](x, w1, w2)\n",
    "            #print('resnet:',i,x.shape)\n",
    "        \n",
    "        #print('final resnet:',x.shape)\n",
    "        \n",
    "        #x = self.global_avg(x)\n",
    "        #print('avg pool:',x.shape)\n",
    "        x = self.final(x.view(-1,256))\n",
    "        x = x.view(-1,1,self.patch_size,self.patch_size)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486bbfd5-2eb2-4bf6-a4cf-c1ed50943990",
   "metadata": {},
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d494f0a2-444d-4335-aeda-fbaaaab35825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dfc3ed-9aac-49e9-82e6-f6df6e1bb248",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_data(patch_size, batch_size, num_anchor):\n",
    "    \n",
    "    X, Y, id_train, id_test = read_data(dir_base_data, dir_super_data)\n",
    "    \n",
    "    si, cluster_per_node, node_per_anchor = spatial_cluster(nnodes, r, z, num_anchor=num_anchor)\n",
    "    \n",
    "    list_trainloaders=[]\n",
    "    list_testloaders =[]\n",
    "    num_batches_tr=[]\n",
    "    num_batches_te=[]\n",
    "    #print(len(X_train), len(X_test), len(Y_train), len(Y_test))\n",
    "    for idx in range(num_anchor):\n",
    "        list_ids = node_per_anchor[idx]\n",
    "        list_train =[]\n",
    "        list_test =[]\n",
    "        for ids in list_ids:\n",
    "            if ids in id_train:\n",
    "                list_train.append(ids)\n",
    "            elif ids in id_test:\n",
    "                list_test.append(ids)\n",
    "                \n",
    "        #print('loader:',idx,len(list_ids),len(list_train),len(list_test))\n",
    "        \n",
    "        trainset = XGCSuperDataset(X, Y, list_train, num_anchor, cluster_per_node, \n",
    "                               transform=None, patch_size=patch_size)\n",
    "    \n",
    "        trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=0)\n",
    "        \n",
    "        list_trainloaders.append(trainloader)\n",
    "        num_batches_tr.append(len(trainloader))\n",
    "        \n",
    "    \n",
    "        testset = XGCSuperDataset(X, Y, list_test, num_anchor, cluster_per_node, \n",
    "                                  transform=None, patch_size=patch_size)\n",
    "    \n",
    "        testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=0)\n",
    "        \n",
    "        list_testloaders.append(testloader)\n",
    "        num_batches_te.append(len(testloader))\n",
    "        print('data loader:',idx, len(trainloader),len(testloader))\n",
    "    \n",
    "    #print('data loader:',len(list_trainloaders),len(list_testloaders))\n",
    "    \n",
    "    return list_trainloaders, list_testloaders, num_batches_tr, num_batches_te\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afe9d20-48be-43d8-a3b3-f68c24256203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(device, net, list_testloaders, criterion, num_batches):\n",
    "    correct = 0.\n",
    "    total =0 \n",
    "    num_test=0\n",
    "    for testloader in list_testloaders:\n",
    "        total+= num_batches[num_test]\n",
    "    \n",
    "        for tdata in testloader:\n",
    "            timages, tlabels, S = tdata['X'], tdata['Y'], tdata['S']\n",
    "            tlables = Variable(tlabels.cuda())\n",
    "            timages = Variable(timages.cuda())\n",
    "            si = Variable(S[0].cuda())\n",
    "            toutputs = net(timages.double(),si.double())\n",
    "            predicted = toutputs.cpu().data\n",
    "            error= criterion(predicted,tlabels)\n",
    "            correct+=error.item()\n",
    "        num_test+=1\n",
    "        \n",
    "    return correct, total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e43b2d1-94c3-40a1-b6b7-4716cf5851ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 256\n",
    "save_freq = 20\n",
    "patch_size = 8 \n",
    "print_freq = 20\n",
    "s_dim =5\n",
    "dir_out = 'checkpoint/hypernet-spatial-concentric/'\n",
    "dir_resume = 'checkpoint/hypernet-spatial-concentric/hypernetworks_plasma.pth'\n",
    "dir_base_data = '/gpfs/alpine/world-shared/csc143/jyc/summit/d3d_coarse_small_v2' \n",
    "dir_super_data = '/gpfs/alpine/world-shared/csc143/jyc/summit/d3d_coarse_small_v2_4x' \n",
    "resume = False\n",
    "\n",
    "\n",
    "if not os.path.exists(dir_out):\n",
    "        os.makedirs(dir_out)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7224b65f",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe02bb5-364b-4fcc-b076-53cad941eeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(device, s_dim):\n",
    "    list_trainloaders, list_testloaders, num_batches_tr, num_batches_te = set_data(patch_size, batch_size, s_dim)\n",
    "    \n",
    "    net = PrimaryNetwork(patch_size=patch_size, s_dim = s_dim)\n",
    "    best_accuracy = 10000.\n",
    "\n",
    "    if resume:\n",
    "        ckpt = torch.load(args.dir_resume)\n",
    "        net.load_state_dict(ckpt['net'])\n",
    "        best_accuracy = ckpt['acc']\n",
    "    net = net.double()\n",
    "    net.cuda()\n",
    "\n",
    "    learning_rate = 0.002\n",
    "    weight_decay = 0.0005\n",
    "    milestones = [168000, 336000, 400000, 450000, 550000, 600000]\n",
    "    max_iter = epochs\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=milestones, gamma=0.5)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    total_iter = 0\n",
    "    epoch = 0\n",
    "    #print_freq = args.print_freq\n",
    "    #num_batches=len(trainloader)\n",
    "    loss_file=open(dir_out+'loss.txt','w')\n",
    "    loss_file.write('epoch,loss,val_loss\\n')\n",
    "    start = time.time()\n",
    "    \n",
    "    while total_iter < max_iter:\n",
    "\n",
    "        running_loss = 0.0\n",
    "        epoch_loss=0\n",
    "        epoch=0\n",
    "        num_loaders=0\n",
    "        num_batch =0\n",
    "        \n",
    "        for trainloader in list_trainloaders:\n",
    "            #print('data loader:',len(trainloader))\n",
    "            num_batch+=num_batches_tr[num_loaders]\n",
    "            \n",
    "            for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "                inputs, labels, S = data['X'], data['Y'], data['S']\n",
    "                nid, rid, cid = data['label'],data['rsid'], data['csid']\n",
    "            \n",
    "            \n",
    "                inputs= Variable(inputs.cuda())\n",
    "                labels = Variable(labels.cuda())\n",
    "                si = Variable(S[0].cuda())\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            \n",
    "                outputs = net(inputs.double(), si.double())\n",
    "                #print('train:',inputs.shape, labels.shape, outputs.shape)\n",
    "                \n",
    "                \n",
    "                loss = criterion(outputs, labels) #+ physics_loss()\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                epoch_loss+=loss.item()\n",
    "                \n",
    "                if i % print_freq == (print_freq-1):\n",
    "                    print(\"[Epoch %d, loader %d] Loss: %.4f\" % (total_iter + 1, \n",
    "                                                                       num_loaders + 1, \n",
    "                                                                       running_loss/print_freq))\n",
    "                running_loss = 0.0\n",
    "                \n",
    "            num_loaders+=1\n",
    "            #break\n",
    "        \n",
    "        epoch += 1\n",
    "        \n",
    "        total_iter += 1\n",
    "        \n",
    "        correct, total= validate(device, net, list_testloaders, criterion, num_batches_te)\n",
    "        \n",
    "        epoch_loss/=num_batch\n",
    "        val_loss = correct/total\n",
    "        string1=str(total_iter)+','+str(epoch_loss)+','+str(val_loss)+'\\n'\n",
    "        loss_file.write(string1)\n",
    "        accuracy = (100. * correct) / total\n",
    "        print('After epoch %d, accuracy: %.4f %%' % (total_iter, accuracy))\n",
    "\n",
    "        if accuracy < best_accuracy:\n",
    "            print('Saving model...')\n",
    "            state = {\n",
    "                'net': net.state_dict(),\n",
    "                'acc': accuracy\n",
    "            }\n",
    "            torch.save(state, dir_out+'hypernet_plasma_'+str(total_iter)+'.pth')\n",
    "            best_accuracy = accuracy\n",
    "        \n",
    "    print('Finished Training')\n",
    "    state = {\n",
    "                'net': net.state_dict(),\n",
    "                'acc': accuracy\n",
    "            }\n",
    "    torch.save(state, dir_out+'last.pth')\n",
    "    loss_file.close()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921c8315-8308-4061-ba04-8975ec03de63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "train(device, s_dim)\n",
    "end = time.time()\n",
    "\n",
    "total = end-start\n",
    "print('Finished Training in '+str(total)+' sec.')   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8983e3f-cb12-4ff9-82f8-ce05ace6dce7",
   "metadata": {},
   "source": [
    "Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a31956e-a86e-427e-8bb8-d7f0ea6c9bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(batch_size, dir_base_data, dir_super_data, patch_size, num_anchor):\n",
    "    \n",
    "    X, Y, id_train, id_test = read_data(dir_base_data, dir_super_data)\n",
    "    \n",
    "    si, cluster_per_node, node_per_anchor = spatial_cluster(nnodes, r, z, num_anchor=num_anchor)\n",
    "    \n",
    "    list_testloaders =[]\n",
    "    num_batches_te=[]\n",
    "    #print(len(X_train), len(X_test), len(Y_train), len(Y_test))\n",
    "    for idx in range(num_anchor):\n",
    "        list_ids = node_per_anchor[idx]\n",
    "        list_test =[]\n",
    "        for ids in list_ids:\n",
    "            if ids in id_test:\n",
    "                list_test.append(ids)\n",
    "                \n",
    "        #print('testloader:',idx,len(list_ids),len(list_test))\n",
    "        \n",
    "    \n",
    "        testset = XGCSuperDataset(X, Y, list_test, num_anchor, cluster_per_node, \n",
    "                                  transform=None, patch_size=patch_size)\n",
    "    \n",
    "        testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=2)\n",
    "        \n",
    "        list_testloaders.append(testloader)\n",
    "        num_batches_te.append(len(testloader))\n",
    "    \n",
    "    \n",
    "    return list_testloaders, X, Y, id_test, num_batches_te\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e300562b-baf2-4d53-a2ce-d50c0f537a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(device, dir_model, s_dim):\n",
    "    net = PrimaryNetwork(patch_size=patch_size, s_dim=s_dim)\n",
    "    ckpt = torch.load(dir_model, map_location=device)\n",
    "    state_dict = ckpt['net']\n",
    "    \n",
    "    new_state_dict = {}\n",
    "    \n",
    "    for k, v in state_dict.items():\n",
    "        k = k.replace(\"module.\", \"\")\n",
    "        new_state_dict[k] = v\n",
    "        state_dict = new_state_dict\n",
    "    \n",
    "    net.to(device=device)\n",
    "    net.load_state_dict(state_dict)\n",
    "    \n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3701061-165b-4da7-9b29-8573e6f7fff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(map_pred_img,predictions, rid, cid, labels, patch_size):\n",
    "    \n",
    "    for l in range(0,len(labels)):\n",
    "        #print(labels[l])\n",
    "        map_pred_img[labels[l]][rid[l]:rid[l]+patch_size,cid[l]:cid[l]+patch_size] = predictions[l,:,:]\n",
    "        \n",
    "    return map_pred_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78504566-78ca-4dba-a4f2-33e379807561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29d702c-9a34-4155-98f4-3d9f7795ae53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(device, batch_size, patch_size, dir_out, dir_model, dir_base_data, dir_super_data, num_anchor):\n",
    "    model = load_model(device, dir_model, num_anchor)\n",
    "    model = model.double()\n",
    "    list_testloaders, Xtest, Ytest, id_test, num_batches_te = load_data(batch_size, \n",
    "                                    dir_base_data, dir_super_data, patch_size, num_anchor)\n",
    "    model.eval()\n",
    "    map_pred_img_ensemble={}\n",
    "    \n",
    "    for tid in id_test:\n",
    "        #print('test labels:',tid[0])\n",
    "        map_pred_img_ensemble[tid[0]]= np.zeros((40,40))\n",
    "    \n",
    "    for testloader in list_testloaders:\n",
    "        #print('testloader:',len(testloader))\n",
    "        for i, data in enumerate(testloader,0):\n",
    "            timages, tlabels, S = data['X'], data['Y'], data['S']\n",
    "            nid, rid, cid = data['label'],data['rsid'], data['csid']\n",
    "            timages = Variable(timages.cuda())\n",
    "            si = Variable(S[0].cuda()) \n",
    "            with torch.no_grad():\n",
    "                toutputs = model(timages.double(),si.double())\n",
    "                predicted = toutputs.cpu().data\n",
    "        \n",
    "            predicted = predicted.squeeze()\n",
    "            predicted = predicted.numpy()\n",
    "            tids = list(nid.numpy())\n",
    "            rid = list(rid.numpy())\n",
    "            cid = list(cid.numpy())\n",
    "            #print(i,predicted.shape,orig_image.shape,len(rid),len(cid),len(tlabels))\n",
    "        \n",
    "            map_pred_img_ensemble = aggregate(map_pred_img_ensemble,predicted, rid, cid, \n",
    "                                          tids, patch_size)\n",
    "    \n",
    "    fname = 'rmse_xgc_test_'+str(batch_size)+'_'+str(len(testloader))+'.txt'\n",
    "    error_file=open(dir_out+fname,'w')\n",
    "    total_err =0\n",
    "    Ypred =[]\n",
    "    for l in range(0,len(id_test)):\n",
    "        tid = id_test[l][0]\n",
    "        targets = map_pred_img_ensemble[tid]\n",
    "        #loss = np.mean((Xtest[l] - targets)**2)\n",
    "        loss = rmse(targets, Ytest[l][0])\n",
    "        total_err+=loss\n",
    "        #print(tid,loss,Xtest[l][0].shape, targets.shape)\n",
    "        string=str(tid)+','+str(loss.item())+'\\n'\n",
    "        error_file.write(string)\n",
    "        Ypred.append(targets)\n",
    "        \n",
    "    print('total:',total_err/len(id_test))\n",
    "    string='total,'+str(total_err)+'\\n'\n",
    "    error_file.write(string)\n",
    "        \n",
    "    error_file.close()\n",
    "    \n",
    "    return Xtest, Ytest, Ypred, id_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f507f23b-9499-4e12-ae43-7d97fd8cd4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "patch_size = 8\n",
    "s_dim = 5\n",
    "dir_out = 'results/hypernet-spatial-concentric/'\n",
    "dir_model = 'checkpoint/hypernet-spatial-concentric/hypernet_plasma_96.pth'\n",
    "dir_base_data = '/gpfs/alpine/world-shared/csc143/jyc/summit/d3d_coarse_small_v2'\n",
    "dir_super_data = '/gpfs/alpine/world-shared/csc143/jyc/summit/d3d_coarse_small_v2_4x'\n",
    "if not os.path.exists(dir_out):\n",
    "        os.makedirs(dir_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a85ed70-aaf4-469b-9ee5-e264456f7c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "xtest, ytest, ypred, idtest = test(device,batch_size, patch_size, dir_out, dir_model,\n",
    "                                   dir_base_data, dir_super_data, s_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51905033-4884-43e8-9e0f-a1eeb98f006e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in range(0,len(idtest)):\n",
    "    np.save(dir_out+'f_pred_'+str(idtest[l][0]),ypred[l])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808d2f5e-0f4e-411c-be9b-5f8a5951aff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_, ny, nx = ytest[0].shape\n",
    "ix = np.linspace(0, nx-1, nx)\n",
    "iy = np.linspace(0, ny-1, ny)\n",
    "Mx, My = np.meshgrid(ix, iy)\n",
    "\n",
    "for l in range(0,len(ytest)):\n",
    "    #plt.figure(figsize=(4, 9))\n",
    "    print('idx:',l)\n",
    "    f = plt.figure(figsize=(12, 8))\n",
    "    f.add_subplot(1,3, 1)\n",
    "    plt.imshow(xtest[l][0], origin='lower')\n",
    "    #plt.colorbar()\n",
    "    plt.contour(Mx, My, ytest[l][0], levels=5, origin='image', colors='white', alpha=0.5)\n",
    "    plt.axis('scaled')\n",
    "    plt.axis('off')\n",
    "    plt.title('input %d'%(idtest[l][0]))\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    f.add_subplot(1,3, 2)\n",
    "    plt.imshow(ytest[l][0], origin='lower')\n",
    "    #plt.colorbar()\n",
    "    plt.contour(Mx, My, ytest[l][0], levels=5, origin='image', colors='white', alpha=0.5)\n",
    "    plt.axis('scaled')\n",
    "    plt.axis('off')\n",
    "    plt.title('original %d'%(idtest[l][0]))\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    f.add_subplot(1,3, 3)\n",
    "    plt.imshow(ypred[l], origin='lower')\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    plt.contour(Mx, My, ypred[l], levels=5, origin='image', colors='white', alpha=0.5)\n",
    "    #RMSE = rmse(ypred[l], ytest[l][0])\n",
    "    #plt.text(.02,.95,'RMSE: {:.04f}'.format(RMSE), fontsize=14, c='white')\n",
    "    plt.axis('scaled')\n",
    "    plt.axis('off')\n",
    "    plt.title('predicted %d'%(idtest[l][0]))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    if l>100:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840477ed",
   "metadata": {},
   "source": [
    "# Plot Error across Mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53efe0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all(base_data_dir, super_data_dir, num_channels=1):\n",
    "    Z0, Zif, zmu, zsig, zmin, zmax, zlb = read_f0(420, expdir=base_data_dir, iphi=0)\n",
    "    Z0_s, Zif_s, zmu_s, zsig_s, zmin_s, zmax_s, zlb_s = read_f0(420, expdir=super_data_dir, iphi=0)\n",
    "    #print('base:',Zif.shape, zlb.shape, zmu.shape, zsig.shape)\n",
    "    #print('super:',Zif_s.shape, zlb_s.shape, zmu_s.shape, zsig_s.shape)\n",
    "    \n",
    "    lx = list()\n",
    "    ly = list()\n",
    "    lid = list()\n",
    "    for i in range(0,len(Zif),num_channels):\n",
    "        X = Zif[i:i+num_channels,:,:]\n",
    "        lx.append(X)\n",
    "        lid.append(zlb[i:i+num_channels])\n",
    "    \n",
    "    #print('lid:',lid[:10])\n",
    "    for ids in lid:\n",
    "        X= Zif_s[ids[0]:ids[0]+num_channels,:,:]\n",
    "        ly.append(X)\n",
    "    \n",
    "    \n",
    "    return lx, ly, lid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16935ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all(batch_size, dir_base_data, dir_super_data, patch_size, num_anchor):\n",
    "    X_test, Y_test, id_test = read_all(dir_base_data, dir_super_data)\n",
    "    \n",
    "    si, cluster_per_node, node_per_anchor = spatial_cluster(nnodes, r, z, num_anchor=num_anchor)\n",
    "    \n",
    "    list_testloaders =[]\n",
    "    \n",
    "    num_batches_te=[]\n",
    "    #print(len(X_train), len(X_test), len(Y_train), len(Y_test))\n",
    "    for idx in range(num_anchor):\n",
    "        list_ids = node_per_anchor[idx]\n",
    "        '''\n",
    "        list_test =[]\n",
    "        for ids in list_ids:\n",
    "            if ids in id_test:\n",
    "                list_test.append(ids)\n",
    "        '''       \n",
    "        #print('testloader:',idx,len(list_ids),len(list_test))\n",
    "        \n",
    "    \n",
    "        testset = XGCSuperDataset(X_test, Y_test, list_ids, num_anchor, cluster_per_node, \n",
    "                                  transform=None, patch_size=patch_size)\n",
    "    \n",
    "        testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=2)\n",
    "        \n",
    "        list_testloaders.append(testloader)\n",
    "        num_batches_te.append(len(testloader))\n",
    "    \n",
    "    \n",
    "    return list_testloaders, X_test, Y_test, id_test, num_batches_te\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cde0d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_all(device, batch_size, patch_size, dir_out, dir_model, dir_base_data, dir_super_data, s_dim):\n",
    "    model = load_model(device, dir_model, s_dim)\n",
    "    model = model.double()\n",
    "    list_testloaders, Xtest, Ytest, id_test, num_batches_te = load_all(batch_size, dir_base_data, dir_super_data, \n",
    "                                                                       patch_size, s_dim)\n",
    "    model.eval()\n",
    "    map_pred_img_ensemble={}\n",
    "    \n",
    "    for tid in id_test:\n",
    "        #print('test labels:',tid[0])\n",
    "        map_pred_img_ensemble[tid[0]]= np.zeros((40,40))\n",
    "    \n",
    "    \n",
    "    for testloader in list_testloaders:\n",
    "        for i, data in enumerate(testloader,0):\n",
    "            timages, tlabels, S = data['X'], data['Y'], data['S']\n",
    "            nid, rid, cid = data['label'],data['rsid'], data['csid']\n",
    "            #timages = timages.to(device=device, dtype=torch.float32)\n",
    "            timages= Variable(timages.cuda())\n",
    "            si = Variable(S[0].cuda())\n",
    "            with torch.no_grad():\n",
    "                toutputs = model(timages.double(),si.double())\n",
    "                predicted = toutputs.cpu().data\n",
    "        \n",
    "            predicted = predicted.squeeze()\n",
    "            predicted = predicted.numpy()\n",
    "            tids = list(nid.numpy())\n",
    "            rid = list(rid.numpy())\n",
    "            cid = list(cid.numpy())\n",
    "            \n",
    "            map_pred_img_ensemble = aggregate(map_pred_img_ensemble,predicted, rid, cid, \n",
    "                                          tids, patch_size)\n",
    "    \n",
    "    fname = 'rmse_all_'+str(batch_size)+'.txt'\n",
    "    error_file=open(dir_out+fname,'w')\n",
    "    total_err =0\n",
    "    Ypred =[]\n",
    "    for l in range(0,len(id_test)):\n",
    "        tid = id_test[l][0]\n",
    "        targets = map_pred_img_ensemble[tid]\n",
    "        #loss = np.mean((Xtest[l] - targets)**2)\n",
    "        loss = rmse(targets, Ytest[l][0])\n",
    "        total_err+=loss\n",
    "        #print(tid,loss,Xtest[l][0].shape, targets.shape)\n",
    "        string=str(tid)+','+str(loss.item())+'\\n'\n",
    "        error_file.write(string)\n",
    "        Ypred.append(targets)\n",
    "        \n",
    "    print('total:',total_err/len(id_test))\n",
    "    string='total,'+str(total_err)+'\\n'\n",
    "    error_file.write(string)\n",
    "        \n",
    "    error_file.close()\n",
    "    \n",
    "    return Xtest, Ytest, Ypred, id_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963f7a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "xtest, ytest, ypred, idtest = test_all(device,batch_size, patch_size, dir_out, dir_model,\n",
    "                                   dir_base_data, dir_super_data,s_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53861ad4-7946-45dc-bb1a-38c46214b970",
   "metadata": {},
   "source": [
    "Plot all rmse error for mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb863e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "result_dir= 'results/hypernet-spatial-concentric/rmse_all_256.txt'\n",
    "rmse={}\n",
    "#rmse_test={}\n",
    "#rmse_train={}\n",
    "with open(result_dir) as f:\n",
    "    for line in f:\n",
    "        node, rmse_sc = line.split(',')\n",
    "        if node!='total':\n",
    "            node = int(node)\n",
    "            rmse_sc = float(rmse_sc)\n",
    "            rmse[node] = rmse_sc\n",
    "        \n",
    "        #num_lines+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86a8242-d31b-415e-9e98-2c3eaf157918",
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_rmse = list(rmse.values())\n",
    "normed_rmse = np.array(normed_rmse)\n",
    "#tmp = (normed_rmse-np.min(normed_rmse))/(np.max(normed_rmse)-np.min(normed_rmse))\n",
    "\n",
    "print(normed_rmse.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e880a8-a64b-4bca-8257-4ebd94cbe493",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#import colormaps as cmaps\n",
    "from matplotlib import colors\n",
    "import matplotlib.tri as tri\n",
    "\n",
    "print(r.shape, z.shape,normed_rmse.shape, nnodes)\n",
    "\n",
    "plt.figure(figsize=[10,16])\n",
    "trimesh = tri.Triangulation(r, z, conn)\n",
    "plt.triplot(trimesh, alpha=0.3)\n",
    "plt.xlabel('R[m]')\n",
    "plt.ylabel('Z[m]')\n",
    "plt.title('XGC mesh')\n",
    "plt.axis('scaled')\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.set_cmap(cmaps.viridis)\n",
    "norm = colors.Normalize(np.min(normed_rmse), np.max(normed_rmse))\n",
    "nc = plt.cm.hot(norm(normed_rmse))\n",
    "for i in range(0,len(normed_rmse)):\n",
    "  #nc = plt.cm.viridis(norm(normed_rmse[i]))\n",
    "  #print(i,r[i],z[i], nc[i])\n",
    "  plt.plot(r[i], z[i], color =nc[i,:], marker = 'o', lw=2)\n",
    "  #plt.text(r[i], z[i], i)\n",
    "\n",
    "sm = plt.cm.ScalarMappable(cmap=plt.cm.hot, norm=colors.Normalize(np.min(normed_rmse), np.max(normed_rmse)))\n",
    "plt.colorbar(sm)\n",
    "plt.savefig('results/hypernet-spatial-concentric/mesh_error_all.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d563fe82-cbfa-4c2d-aded-08382a4845ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OLCF-CUDA11 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
